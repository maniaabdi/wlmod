{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import graph_tool.all as gt\n",
    "from datetime import datetime, date, time\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "statistics_dir = '/home/maniaa/ashes/code/statistics/'\n",
    "stat_csv = [(statistics_dir + f) for f in listdir(statistics_dir) if (f.endswith(\".csv\") and isfile(join(statistics_dir, f)))]\n",
    "stat_csv.sort()\n",
    "stat_csv = stat_csv[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/maniaa/ashes/code/statistics/08-02-2018.csv\n"
     ]
    }
   ],
   "source": [
    "for index, f in enumerate(stat_csv):\n",
    "    # 6Am of each day\n",
    "    print(f)\n",
    "    month, day, year = f.split('/')[-1].split('.csv')[0].split('-')\n",
    "    trace_starttime = datetime.combine(date(int(year), int(month), int(day)), time(7, 0))\n",
    "    \n",
    "    df = pd.read_csv(f)\n",
    "    df = df[df['submitTime']/1000 > datetime.timestamp(trace_starttime)]\n",
    "    \n",
    "    df['submit_ts'] = df['submitTime']//1000 - datetime.timestamp(trace_starttime);\n",
    "    df.sort_values('submit_ts', inplace=True)\n",
    "    df.reset_index(inplace=True)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['index', 'state', 'submitTime', 'startTime', 'finishTime', 'queueTime',\n",
      "       'runTime', 'NumMaps', 'avgMapTime', 'avgReduceTime', 'avgShuffleTime',\n",
      "       'avgMergeTime', 'NumReduce', 'HDFS_INPUT_SIZE', 'HDFS_OUTPUT_SIZE',\n",
      "       'MAP_CPU_USAGE_MSEC', 'REDUCE_CPU_USAGE_MSEC', 'MAP_MEM_USAGE_B',\n",
      "       'REDUCE_MEM_USAGE_B', 'HIVE_RECORDS_IN', 'HIVE_RECORDS_OUT',\n",
      "       'HIVE_RECORDS_INTERMEDIATE', 'SLOTS_MILLIS_MAPS',\n",
      "       'SLOTS_MILLIS_REDUCES', 'TOTAL_LAUNCHED_MAPS', 'TOTAL_LAUNCHED_REDUCES',\n",
      "       'DATA_LOCAL_MAPS', 'RACK_LOCAL_MAPS', 'MILLIS_MAPS', 'MILLIS_REDUCES',\n",
      "       'VCORES_MILLIS_MAPS', 'VCORES_MILLIS_REDUCES', 'MB_MILLIS_MAPS',\n",
      "       'MB_MILLIS_REDUCES', 'PHMAP_MEM_USAGE_B', 'PHREDUCE_MEM_USAGE_B',\n",
      "       'PHPHYSICAL_MEMORY_B', 'jobid', 'job.maps', 'query', 'outputdir',\n",
      "       'scratchdir', 'sessionid', 'query.id', 'local.scratchdir', 'tmpouput',\n",
      "       'user.name', 'job', 'n_inputs', 'inputdir', 'workflow.node',\n",
      "       'workflow.id', 'workflow.dag', 'table.name', 'submit_ts'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {}\n",
    "\n",
    "def get_obj(f):\n",
    "    f = f.replace('hdfs://datalake-nnha', '')\n",
    "    if '-ext-' in f:\n",
    "        return f.rsplit('-', 1)[0]\n",
    "    elif '-mr-' in f:\n",
    "        return f.rsplit('/', 1)[0]\n",
    "    return f;\n",
    "    \n",
    "\n",
    "def match_outputs(data):\n",
    "    global metadata\n",
    "    o_obj = get_obj(data['tmpouput'])\n",
    "    iobjs = [get_obj(f) for f in data['inputdir'].split(',')]\n",
    "    if o_obj in iobjs:\n",
    "        o_obj = o_obj.split('.hive')[0]+data['table.name'].rsplit('.')[1]\n",
    "                \n",
    "    if o_obj not in metadata:\n",
    "        metadata[o_obj] = {'id': len(metadata), 'size': data['HDFS_OUTPUT_SIZE'], 'reuse': 0, \n",
    "                           'name': o_obj, 'approximate': 0, 'jobs': []}\n",
    "    metadata[o_obj]['reuse'] += 1;\n",
    "    metadata[o_obj]['jobs'].append(data['jobid'])\n",
    "    metadata[o_obj]['size'] = data['HDFS_OUTPUT_SIZE'];\n",
    "    data['outputdir'] = o_obj\n",
    "    return data\n",
    "\n",
    "def approximate_inputs(row):\n",
    "    global metadata\n",
    "    job_meta = {}\n",
    "    for f in row['inputdir'].split(','):\n",
    "        i_obj = get_obj(f)\n",
    "        if i_obj not in job_meta:\n",
    "            job_meta[i_obj] = 0;\n",
    "        job_meta[i_obj] += 1;\n",
    "    \n",
    "    for i_obj in job_meta:\n",
    "        if i_obj not in metadata: \n",
    "            metadata[i_obj] = {'id': len(metadata), 'name': i_obj, 'reuse': 0, 'approximate': 1,\n",
    "                               'size': job_meta[i_obj]*(row['HDFS_INPUT_SIZE']//row['n_inputs']),\n",
    "                              'jobs': []}\n",
    "        metadata[i_obj]['reuse'] += 1;\n",
    "        metadata[i_obj]['jobs'].append(row['jobid'])\n",
    "        \n",
    "    row['inputdir'] = ','.join(job_meta.keys())\n",
    "    row['n_inputs'] = sum(job_meta.values())\n",
    "    return row\n",
    "\n",
    "def check_and_assign_size(data):\n",
    "    global metadata\n",
    "    \n",
    "    if len(data['inputdir']) == 0:\n",
    "        return data;    \n",
    "    \n",
    "    iobjs = [get_obj(f) for f in data['inputdir'].split(',')]\n",
    "    if len(iobjs) > 1:\n",
    "        return data;\n",
    "    \n",
    "    for f in iobjs:    \n",
    "        if f not in metadata:\n",
    "            metadata[f] = {'id': len(metadata), 'size': data['HDFS_INPUT_SIZE'], 'reuse': 0, \n",
    "                           'name': f, 'approximate': 0}\n",
    "        metadata[f]['reuse'] += 1\n",
    "        \n",
    "    data['inputdir'] = ''\n",
    "    data['HDFS_INPUT_SIZE'] = 0\n",
    "    return data\n",
    "\n",
    "def remove_observed(data):\n",
    "    global metadata\n",
    "    del_ls = []\n",
    "    inputs = data['inputdir'].split(',')\n",
    "    remove_size = 0\n",
    "    data_size = data['HDFS_INPUT_SIZE']\n",
    "    if data_size == 0: return data\n",
    "    \n",
    "    input_sz = data_size\n",
    "    \n",
    "    for f in inputs:\n",
    "        if get_obj(f) in metadata:\n",
    "            metadata[get_obj(f)]['reuse'] +=1\n",
    "            del_ls.append(f)\n",
    "            remove_size +=  metadata[get_obj(f)]['size']\n",
    "    for f in del_ls:\n",
    "        inputs.remove(f)\n",
    "    \n",
    "    if data['HDFS_INPUT_SIZE'] < remove_size:\n",
    "        data_size = 0;\n",
    "    else:\n",
    "        data_size = data_size - remove_size\n",
    "        \n",
    "    data['HDFS_INPUT_SIZE'] = data_size\n",
    "    data['inputdir'] = ','.join(inputs)\n",
    "    \n",
    "    if len(inputs) == 0:\n",
    "        data['HDFS_INPUT_SIZE'] = 0\n",
    "    \n",
    "    return data\n",
    "\n",
    "def approximate_size_assignment(data):\n",
    "    global metadata\n",
    "    inputs = [get_obj(f) for f in data['inputdir'].split(',')]\n",
    "    n_splits = len(inputs)\n",
    "    split_size = data['HDFS_INPUT_SIZE']/n_splits\n",
    "    \n",
    "    for f in inputs:\n",
    "        if f not in metadata:\n",
    "            metadata[f] = {'id': len(metadata), 'size': split_size, 'reuse': 0,\n",
    "                          'name': f, 'approximate': 0} \n",
    "        metadata[f]['reuse'] += 1\n",
    "    data['inputdir'] = ''\n",
    "    data['HDFS_INPUT_SIZE'] = 0\n",
    "    return data  \n",
    "        \n",
    "\n",
    "def identify_obj_size(idx, g):\n",
    "    for i in range(0, idx+1):\n",
    "        g = g.apply(remove_observed, axis=1)\n",
    "        g = g.apply(check_and_assign_size, axis=1)\n",
    "    if g[g['HDFS_INPUT_SIZE'] != 0].empty == False:\n",
    "        g = g[g['HDFS_INPUT_SIZE'] != 0].apply(approximate_size_assignment, axis=1)\n",
    "    print(idx, len(metadata))        \n",
    "    \n",
    "df = df.apply(match_outputs, axis=1)    \n",
    "df = df.apply(approximate_inputs, axis=1)    \n",
    "#grps = df.groupby('n_inputs')\n",
    "\n",
    "#for idx, g in grps:\n",
    "#    identify_obj_size(idx, g)\n",
    "#    \n",
    "#    for i in range(0, idx):\n",
    "#        if g.empty == True: continue;\n",
    "#        g = g.apply(print_name2, axis=1)\n",
    "#        g = g.apply(print_name, axis=1)\n",
    "\n",
    "#    if g[g['HDFS_INPUT_SIZE'] != 0].empty == False:\n",
    "#        \n",
    "#    print(idx, len(metadata))\n",
    "#    #if idx > 20:\n",
    "#    #    break;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>size</th>\n",
       "      <th>reuse</th>\n",
       "      <th>name</th>\n",
       "      <th>approximate</th>\n",
       "      <th>jobs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13621</td>\n",
       "      <td>74981601</td>\n",
       "      <td>2714</td>\n",
       "      <td>/hive_external_tables/SYSTEM/dt=20180731</td>\n",
       "      <td>1</td>\n",
       "      <td>[job_1531656020138_222164, job_1531656020138_222173, job_1531656020138_222176, job_1531656020138...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13567</td>\n",
       "      <td>43231231</td>\n",
       "      <td>1746</td>\n",
       "      <td>/hive_external_tables/ASUP/dt=20180731</td>\n",
       "      <td>1</td>\n",
       "      <td>[job_1531656020138_222152, job_1531656020138_222171, job_1531656020138_222180, job_1531656020138...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13602</td>\n",
       "      <td>19695698</td>\n",
       "      <td>1733</td>\n",
       "      <td>/hive_external_tables/CLUSTER_TABLE/dt=20180731</td>\n",
       "      <td>1</td>\n",
       "      <td>[job_1531656020138_222157, job_1531656020138_222172, job_1531656020138_222184, job_1531656020138...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11950</td>\n",
       "      <td>56802497</td>\n",
       "      <td>271</td>\n",
       "      <td>/hive_external_tables/CLUSTER_MEMBER/dt=20180731</td>\n",
       "      <td>1</td>\n",
       "      <td>[job_1531656020138_222148, job_1531656020138_222168, job_1531656020138_222178, job_1531656020138...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>77306</td>\n",
       "      <td>75141273</td>\n",
       "      <td>205</td>\n",
       "      <td>/hive_external_tables/SYSTEM/dt=20180801</td>\n",
       "      <td>1</td>\n",
       "      <td>[job_1531656020138_232288, job_1531656020138_232320, job_1531656020138_232327, job_1531656020138...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86143</th>\n",
       "      <td>5576</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>/hive_external_tables/Users_db/guestdb/ubs_shelf_asups_sd/.hive-staging_hive_2018-08-02_09-17-09...</td>\n",
       "      <td>0</td>\n",
       "      <td>[job_1531656020138_228038]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86144</th>\n",
       "      <td>5575</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>/tmp/hive/netappiq/0d152597-7ee6-42b2-aac7-8de1b71cef1c/hive_2018-08-02_09-17-27_658_83272760737...</td>\n",
       "      <td>0</td>\n",
       "      <td>[job_1531656020138_228037]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86145</th>\n",
       "      <td>5574</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>/tmp/hive/netappiq/276fcb2d-e1fb-4e61-a9d7-907c2a5d4443/hive_2018-08-02_09-17-21_525_75390598473...</td>\n",
       "      <td>0</td>\n",
       "      <td>[job_1531656020138_228036]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86146</th>\n",
       "      <td>5573</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>/tmp/hive/netappiq/61b2aaa9-5cd2-4c4a-aef3-4d470d25f1d1/hive_2018-08-02_09-17-18_444_66666308392...</td>\n",
       "      <td>0</td>\n",
       "      <td>[job_1531656020138_228035]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86147</th>\n",
       "      <td>86147</td>\n",
       "      <td>7204</td>\n",
       "      <td>1</td>\n",
       "      <td>/tmp/hive/asupdl/7389e985-b292-4ff7-b969-1884bcf82198/hive_2018-08-02_23-58-57_021_8844374040109...</td>\n",
       "      <td>1</td>\n",
       "      <td>[job_1531656020138_234736]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86148 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id      size  reuse  \\\n",
       "0      13621  74981601   2714   \n",
       "1      13567  43231231   1746   \n",
       "2      13602  19695698   1733   \n",
       "3      11950  56802497    271   \n",
       "4      77306  75141273    205   \n",
       "...      ...       ...    ...   \n",
       "86143   5576        52      1   \n",
       "86144   5575         3      1   \n",
       "86145   5574        17      1   \n",
       "86146   5573        17      1   \n",
       "86147  86147      7204      1   \n",
       "\n",
       "                                                                                                      name  \\\n",
       "0                                                                 /hive_external_tables/SYSTEM/dt=20180731   \n",
       "1                                                                   /hive_external_tables/ASUP/dt=20180731   \n",
       "2                                                          /hive_external_tables/CLUSTER_TABLE/dt=20180731   \n",
       "3                                                         /hive_external_tables/CLUSTER_MEMBER/dt=20180731   \n",
       "4                                                                 /hive_external_tables/SYSTEM/dt=20180801   \n",
       "...                                                                                                    ...   \n",
       "86143  /hive_external_tables/Users_db/guestdb/ubs_shelf_asups_sd/.hive-staging_hive_2018-08-02_09-17-09...   \n",
       "86144  /tmp/hive/netappiq/0d152597-7ee6-42b2-aac7-8de1b71cef1c/hive_2018-08-02_09-17-27_658_83272760737...   \n",
       "86145  /tmp/hive/netappiq/276fcb2d-e1fb-4e61-a9d7-907c2a5d4443/hive_2018-08-02_09-17-21_525_75390598473...   \n",
       "86146  /tmp/hive/netappiq/61b2aaa9-5cd2-4c4a-aef3-4d470d25f1d1/hive_2018-08-02_09-17-18_444_66666308392...   \n",
       "86147  /tmp/hive/asupdl/7389e985-b292-4ff7-b969-1884bcf82198/hive_2018-08-02_23-58-57_021_8844374040109...   \n",
       "\n",
       "       approximate  \\\n",
       "0                1   \n",
       "1                1   \n",
       "2                1   \n",
       "3                1   \n",
       "4                1   \n",
       "...            ...   \n",
       "86143            0   \n",
       "86144            0   \n",
       "86145            0   \n",
       "86146            0   \n",
       "86147            1   \n",
       "\n",
       "                                                                                                      jobs  \n",
       "0      [job_1531656020138_222164, job_1531656020138_222173, job_1531656020138_222176, job_1531656020138...  \n",
       "1      [job_1531656020138_222152, job_1531656020138_222171, job_1531656020138_222180, job_1531656020138...  \n",
       "2      [job_1531656020138_222157, job_1531656020138_222172, job_1531656020138_222184, job_1531656020138...  \n",
       "3      [job_1531656020138_222148, job_1531656020138_222168, job_1531656020138_222178, job_1531656020138...  \n",
       "4      [job_1531656020138_232288, job_1531656020138_232320, job_1531656020138_232327, job_1531656020138...  \n",
       "...                                                                                                    ...  \n",
       "86143                                                                           [job_1531656020138_228038]  \n",
       "86144                                                                           [job_1531656020138_228037]  \n",
       "86145                                                                           [job_1531656020138_228036]  \n",
       "86146                                                                           [job_1531656020138_228035]  \n",
       "86147                                                                           [job_1531656020138_234736]  \n",
       "\n",
       "[86148 rows x 6 columns]"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option(\"display.max_colwidth\", 100)\n",
    "pd.set_option(\"display.max_rows\", 40)\n",
    "metadata_df = pd.DataFrame(data=metadata.values()).set_index('id')\n",
    "dsr = metadata_df.sort_values('reuse', ascending=False).reset_index()\n",
    "dsr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203500065114\n",
      "52    74981601\n",
      "Name: HDFS_INPUT_SIZE, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(dsr.loc[0]['size']*dsr.loc[0]['reuse'])\n",
    "dt = pd.DataFrame()\n",
    "dt['reuse_distance'] = df[df['jobid'].isin(dsr.loc[0]['jobs'])].set_index('index').sort_values('submit_ts').reset_index()['submit_ts'].diff()\n",
    "dt[['submit_ts', 'n_inputs', 'jobid']] = df[df['jobid'].isin(dsr.loc[0]['jobs'])].set_index('index').sort_values('submit_ts').reset_index()[['submit_ts', 'n_inputs', 'jobid']]\n",
    "\n",
    "dt\n",
    "\n",
    "print(df[df['jobid'] == 'job_1531656020138_222200']['HDFS_INPUT_SIZE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
