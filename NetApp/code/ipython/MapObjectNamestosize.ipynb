{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['state', 'submitTime', 'startTime', 'finishTime', 'queueTime',\n",
      "       'runTime', 'NumMaps', 'avgMapTime', 'avgReduceTime', 'avgShuffleTime',\n",
      "       'avgMergeTime', 'NumReduce', 'HDFS_INPUT_SIZE', 'HDFS_OUTPUT_SIZE',\n",
      "       'MAP_CPU_USAGE_MSEC', 'REDUCE_CPU_USAGE_MSEC', 'MAP_MEM_USAGE_B',\n",
      "       'REDUCE_MEM_USAGE_B', 'HIVE_RECORDS_IN', 'HIVE_RECORDS_OUT',\n",
      "       'HIVE_RECORDS_INTERMEDIATE', 'SLOTS_MILLIS_MAPS',\n",
      "       'SLOTS_MILLIS_REDUCES', 'TOTAL_LAUNCHED_MAPS', 'TOTAL_LAUNCHED_REDUCES',\n",
      "       'DATA_LOCAL_MAPS', 'RACK_LOCAL_MAPS', 'MILLIS_MAPS', 'MILLIS_REDUCES',\n",
      "       'VCORES_MILLIS_MAPS', 'VCORES_MILLIS_REDUCES', 'MB_MILLIS_MAPS',\n",
      "       'MB_MILLIS_REDUCES', 'PHMAP_MEM_USAGE_B', 'PHREDUCE_MEM_USAGE_B',\n",
      "       'PHPHYSICAL_MEMORY_B', 'jobid', 'job.maps', 'query', 'outputdir',\n",
      "       'scratchdir', 'sessionid', 'query.id', 'local.scratchdir', 'user.name',\n",
      "       'job', 'n_inputs', 'inputdir', 'workflow.node', 'workflow.id',\n",
      "       'workflow.dag', 'table.name'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "df1 = pd.read_csv('code/statistics/08-04-2018.csv')\n",
    "df = df1[df1['state'] == 'SUCCEEDED']\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221 124361654 104720361 1\n",
      "652 245076746 -7834 2\n",
      "1149 16694602677 16138744817 1\n",
      "1316 341076683 95992103 2\n",
      "1775 145073740 101393500 1\n",
      "1896 141529143211 140973285351 1\n",
      "1905 245078852 -5728 2\n",
      "2287 245080532 -4048 2\n",
      "2420 245075574 -9006 2\n",
      "2461 317600927538 317045069678 1\n",
      "2710 596206943 40349083 1\n",
      "2921 245076520 -8060 2\n",
      "3509 245076808 -7772 2\n",
      "3628 317600946446 317045088586 1\n",
      "3819 217966455 123521835 2\n",
      "3882 341075198 95990618 2\n",
      "5147 73000513 53359220 1\n",
      "5247 616154339 60296479 1\n",
      "5407 726728256 96067069 2\n",
      "5524 245091658 7078 2\n",
      "5537 1198675559 642817699 1\n",
      "5982 8704243391 8148385531 1\n",
      "6779 1198675591 642817731 1\n",
      "6886 16694602677 16138744817 1\n",
      "7421 1132874064 577016204 1\n",
      "7824 245073060 -11520 2\n",
      "7909 396160509254 395604651394 1\n",
      "7976 185606574771 185050716911 1\n",
      "8235 556145636 287776 1\n"
     ]
    }
   ],
   "source": [
    "gbni = df.groupby('n_inputs')\n",
    "\n",
    "id = 0\n",
    "name_to_size_map = {}\n",
    "\n",
    "group = gbni.get_group(1);\n",
    "for ji, job in group.iterrows():\n",
    "    inputs = job['inputdir'].split(',')\n",
    "    for fi in inputs:\n",
    "        if fi not in name_to_size_map: \n",
    "            name_to_size_map[fi] = {'id': id, 'size': job['HDFS_INPUT_SIZE']}\n",
    "            id += 1;\n",
    "        else:\n",
    "            if name_to_size_map[fi]['size'] > job['HDFS_INPUT_SIZE']:\n",
    "                name_to_size_map[fi]['size'] = job['HDFS_INPUT_SIZE']\n",
    "                \n",
    "group = gbni.get_group(2);\n",
    "for ji, job in group.iterrows():\n",
    "    job_input_size = job['HDFS_INPUT_SIZE']\n",
    "    inputs = job['inputdir'].split(',')\n",
    "    accurate = 0\n",
    "    for fi in inputs:\n",
    "        if fi in name_to_size_map:\n",
    "            accurate += 1\n",
    "            job_input_size -= name_to_size_map[fi]['size']\n",
    "    if job['HDFS_INPUT_SIZE'] != job_input_size:\n",
    "        print(ji, job['HDFS_INPUT_SIZE'], job_input_size, accurate)\n",
    "            \n",
    "            \n",
    "            \n",
    "#for gi, group in gbni.iterrows():\n",
    "#    if gi == 1:\n",
    "#        continue:\n",
    "#    for ji, job in group.iterrows():\n",
    "#        inputs = job['inputdir'].split(',')\n",
    "#        for fi in inputs:\n",
    "#            if "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
