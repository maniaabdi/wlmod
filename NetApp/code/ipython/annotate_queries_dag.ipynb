{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import graph_tool.all as gt\n",
    "from datetime import datetime, date, time\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "statistics_dir = '/home/maniaa/ashes/dataset/statistics/'\n",
    "stat_csv = [(statistics_dir + f) for f in listdir(statistics_dir) if (f.endswith(\".csv\") and isfile(join(statistics_dir, f)))]\n",
    "stat_csv.sort()\n",
    "stat_csv = stat_csv[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/maniaa/ashes/dataset/statistics/08-02-2018.csv\n",
      "Index(['state', 'submitTime', 'startTime', 'finishTime', 'queueTime',\n",
      "       'runTime', 'NumMaps', 'avgMapTime', 'avgReduceTime', 'avgShuffleTime',\n",
      "       'avgMergeTime', 'NumReduce', 'HDFS_INPUT_SIZE', 'HDFS_OUTPUT_SIZE',\n",
      "       'MAP_CPU_USAGE_MSEC', 'REDUCE_CPU_USAGE_MSEC', 'MAP_MEM_USAGE_B',\n",
      "       'REDUCE_MEM_USAGE_B', 'HIVE_RECORDS_IN', 'HIVE_RECORDS_OUT',\n",
      "       'HIVE_RECORDS_INTERMEDIATE', 'SLOTS_MILLIS_MAPS',\n",
      "       'SLOTS_MILLIS_REDUCES', 'TOTAL_LAUNCHED_MAPS', 'TOTAL_LAUNCHED_REDUCES',\n",
      "       'DATA_LOCAL_MAPS', 'RACK_LOCAL_MAPS', 'MILLIS_MAPS', 'MILLIS_REDUCES',\n",
      "       'VCORES_MILLIS_MAPS', 'VCORES_MILLIS_REDUCES', 'MB_MILLIS_MAPS',\n",
      "       'MB_MILLIS_REDUCES', 'PHMAP_MEM_USAGE_B', 'PHREDUCE_MEM_USAGE_B',\n",
      "       'PHPHYSICAL_MEMORY_B', 'jobid', 'job.maps', 'query', 'outputdir',\n",
      "       'scratchdir', 'sessionid', 'query.id', 'local.scratchdir', 'tmpouput',\n",
      "       'user.name', 'job', 'n_inputs', 'inputdir', 'workflow.node',\n",
      "       'workflow.id', 'workflow.dag', 'table.name', 'submit_ts'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "dt = pd.DataFrame()\n",
    "# 90th Percentile\n",
    "def build_graph(gstr):\n",
    "    wrk_edges = [e.split('>') for e in gstr.split(',')]\n",
    "    lbl_vid = {}\n",
    "    graph = {'nodes': {}, 'edges': []}\n",
    "    g = gt.Graph(directed=True)\n",
    "    v_lbl = g.new_vertex_property(\"int\")\n",
    "    for e in wrk_edges:\n",
    "        srclbl = int(e[0].split('-')[1])\n",
    "        if srclbl not in lbl_vid:\n",
    "            vsrc = g.add_vertex()\n",
    "            v_lbl[vsrc] = srclbl\n",
    "            lbl_vid[srclbl] = int(vsrc)\n",
    "            \n",
    "        if len(e) > 1:\n",
    "            trgtlbl = int(e[1].split('-')[1])\n",
    "            if trgtlbl not in lbl_vid:\n",
    "                vtgt = g.add_vertex()\n",
    "                v_lbl[vtgt] = trgtlbl\n",
    "                lbl_vid[trgtlbl] = int(vtgt)\n",
    "            g.add_edge(lbl_vid[srclbl], lbl_vid[trgtlbl])\n",
    "   \n",
    "    g.vertex_properties['label'] = v_lbl\n",
    "    return g\n",
    "    \n",
    "def num_vertices(g):\n",
    "    return g.num_vertices()\n",
    "\n",
    "def num_edges(g):\n",
    "    return g.num_edges()\n",
    " \n",
    "    \n",
    "for index, f in enumerate(stat_csv):\n",
    "    # 6Am of each day\n",
    "    print(f)\n",
    "    month, day, year = f.split('/')[-1].split('.csv')[0].split('-')\n",
    "    trace_starttime = datetime.combine(date(int(year), int(month), int(day)), time(7, 0))\n",
    "    \n",
    "    df = pd.read_csv(f)\n",
    "    df = df[df['submitTime']/1000 > datetime.timestamp(trace_starttime)]\n",
    "    \n",
    "    df = df[df['state'] == 'SUCCEEDED']\n",
    "    df['submit_ts'] = df['submitTime']//1000 - datetime.timestamp(trace_starttime);\n",
    "\n",
    "    print(df.columns)\n",
    "      \n",
    "    dt['dag'] = df.groupby('workflow.id')['workflow.dag'].agg('max').apply(build_graph)\n",
    "    dt[['query', 'submitTime']] = df.groupby('workflow.id')[['query', 'submitTime']].agg('max')\n",
    "    dt['n_v'] = dt['dag'].apply(num_vertices)\n",
    "    dt['n_e'] = dt['dag'].apply(num_edges)\n",
    "    dt.reset_index(inplace=True)\n",
    "    if index==0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.sort_values('submitTime', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = hive.connect(host='localhost').cursor()\n",
    "\n",
    "que = 'drop table default.system'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = '''CREATE table IF NOT EXISTS default.system \n",
    "(dt date, asup_key string, asup_subject string, sys_serial_no string, sys_is_netapp string,\n",
    "asup_id string, sys_version_fbranch string, system_id string, partner_hostname string,\n",
    "partner_system_id string,Hostname string,sys_model string,sys_version string, sys_top_domain string,\n",
    "sys_operating_mode string, sys_type string, sys_domain string, additional_fields  map<string,string>)'''\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'drop table default.cluster_table'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS default.cluster_table (cluster_name string, cluster_identifier string, asup_id string, dt date, cluster_uuid string, asup_key string)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "\n",
    "que = 'drop table default.customerib'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = '''CREATE table IF NOT EXISTS default.customerib (sap_status string, not_e1000_customer_flag string, \n",
    "e1000_customer_flag string, G50_customer_flag string, A50_customer_flag string, msb_customer_flag string, \n",
    "email_address string, serial_number string, customer_id string, parent_name string, customer_name string,\n",
    "site_name string,service_geo string,site_country_name string,site_country string)'''\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'drop table default.asup'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = '''CREATE table IF NOT EXISTS default.asup (asup_subject string, asup_id string, \n",
    "asup_type string, dt date, asup_key string, asup_gen_date date)'''\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'drop table default.volume'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS default.volume (vol_name string, vol_state_asis string, asup_id string, aggr_id string, vol_compressed_kb string, dt date, vol_allocated_kb  string, additional_fields  map<string,string>, vol_is_root  string, vol_is_online  string, vol_asis_saved_kb  string, vol_asis_saved_pct  string, vol_compressed_pct string)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'drop table default.aggregate'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS default.aggregate (aggr_is_hybrid string, aggr_used_kb int, aggr_sused_kb int, dt date, asup_id string, aggr_name string)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'drop table default.es_volume_group'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = '''CREATE table IF NOT EXISTS default.es_volume_group \n",
    "(sys_serial_no string, asup_id string, asup_gen_date date,\n",
    "volumegroupref string, label string, dt date, serial_no string,\n",
    "max_asup_id string, max_dt date)'''\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'drop table default.cm'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = '''CREATE table IF NOT EXISTS default.cm (asup_key string, object_name string, \n",
    "dt date, counters map<string,int>, instance_name string, cur_time_stamp date)'''\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'drop table default.DFM_APP_INFO'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS default.DFM_APP_INFO (dt date)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'drop table default.DFM_APP_MODEL'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS default.DFM_APP_MODEL (dt date)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'drop table oci_svc_consumption_list_details'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = '''CREATE table IF NOT EXISTS oci_svc_consumption_list_details (dt date, sys_serial_no string, \n",
    "consumption_level string, calc_key string, consumed_qt int, service_nm string, weekend_date date)'''\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'drop table default.cluster_member'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS default.cluster_member (asup_id string, dt date)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'drop table system_perf_v1'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = '''CREATE table IF NOT EXISTS system_perf_v1 (ttl_perf_v1_ct int, ttl_diskkb_cp_read_ct int,\n",
    "ttl_diskkb_cp_write_ct int, ttl_diskkb_usr_ct int, asup_id string, dt date,\n",
    "ttl_disk_ops_cp_read_ct int, ttl_disk_ops_cp_write_ct int, ttl_disk_ops_usr_ct int)'''\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'drop table system_health_alert'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = '''CREATE table IF NOT EXISTS system_health_alert (asup_id string, alert_id string, alerting_resource string,\n",
    "dt date, alerting_resource_name string, asup_key string)'''\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'drop table system_fru'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = '''CREATE table IF NOT EXISTS system_fru (netapp_part_no string, serial_no string, asup_key string, fru_name string)'''\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'drop table motherboard'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = '''CREATE table IF NOT EXISTS motherboard (mb_serial_no string, mb_partno string, \n",
    "asup_key string, asup_id string)'''\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'drop table smfdbv2.cifs_share_byname'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = '''CREATE table IF NOT EXISTS smfdbv2.cifs_share_byname (vserver string, asup_id string, dt date)'''\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'drop table default.DFM_GLOBAL_STATUS'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS default.DFM_GLOBAL_STATUS (dt date)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'drop table default.DFM_GUI_CLICK'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS default.DFM_GUI_CLICK (dt date)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'drop table default.software_image'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS default.software_image (system_image_version string, asup_id string, system_image_is_current string, dt date)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'drop table default.adapter'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS default.adapter (hba_name string, asup_id string, dt date)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'drop table event_subscription'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS event_subscription (asup_id string, user_id string, dt date)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'drop table default.DFM_OBJECT'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS default.DFM_OBJECT (asup_id string, user_id string, dt date)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'drop table bart_disk_errors_f'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS bart_disk_errors_f (asup_id string,dt date)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'drop table default.DFM_PERFORMANCE_ADVISOR'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS default.DFM_PERFORMANCE_ADVISOR (asup_id string,dt date)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "que = 'drop table default.EMS_EXPORT'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS default.EMS_EXPORT (asup_id string,dt date)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'drop table crm_data'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS crm_data  (system_id string,  sys_serial_no string, partners  ARRAY<string>, system_state string, customer_id string)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'drop table event_subscription'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS event_subscription  (user_id string, user_name string, crm_cust_list  ARRAY<string>,  cust_list ARRAY<string>, event_id string, is_subscribed string)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    " \n",
    "que = 'drop table renewal_data'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS renewal_data (sys_serial_no string, system_id string, host_name string, hw_contract_id string, hw_contract_end_date date, sw_contract_id string, sw_contract_end_date date, nrd_contract_id date, nrd_contract_end_date date)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'drop table smfdbv2.sysmgr_usage'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS smfdbv2.sysmgr_usage (asup_id string, counterkey string, countervalue string, dt date)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'drop table guestdb.puat_sm_data_final'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS guestdb.puat_sm_data_final (asup_id string, counterkey string, countervalue string, dt date)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'drop table bart_scsicmdchkcon_n'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS bart_scsicmdchkcon_n (asup_id string, dt date)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'drop table manifest'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS manifest (asup_id string, dt date, body_file string, collected_size int)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'drop table guestdb.puat_nsm_ocsm_data'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = '''CREATE table IF NOT EXISTS\n",
    "guestdb.puat_nsm_ocsm_data\n",
    "(counterkey string,\n",
    "countervalue string,\n",
    "asup_id string)'''\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'drop table guestdb.dim_masterpivot_custsite'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = '''CREATE table IF NOT EXISTS\n",
    "guestdb.dim_masterpivot_custsite \n",
    "(Serial_Number  string, Customer_Name string, Parent_Name string, site_country_name string, Service_geo string, \n",
    "g50_customer_flag string, a50_customer_flag string,\n",
    "e1000_customer_flag string,\n",
    "roe_customer_flag string,\n",
    "not_e1000_customer_flag string,\n",
    "msb_customer_flag  string)'''\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "\n",
    "que = 'drop table default.DFM_PROVISIONING_MANAGER_POLICY'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS default.DFM_PROVISIONING_MANAGER_POLICY (asup_id string,dt date)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'drop table default.DFM_SERVER_MANAGEMENT'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS default.DFM_SERVER_MANAGEMENT (asup_id string,dt date)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'drop table default.DFM_PROVISIONING_MANAGER'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS default.DFM_PROVISIONING_MANAGER (asup_id string,dt date)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "\n",
    "que = 'drop table default.DQP_DETAILS'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS default.DQP_DETAILS (asup_id string,dt date)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'drop table default.DFM_SYSTEM_INFO'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS default.DFM_SYSTEM_INFO (asup_id string,dt date)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'drop table default.DFM_SERVICE'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS default.DFM_SERVICE (asup_id string,dt date)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'drop table default.DR_CLUSTER'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS default.DR_CLUSTER (asup_id string,dt date)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'drop table default.DR_CLUSTER_NODES'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS default.DR_CLUSTER_NODES (asup_id string,dt date)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'drop table default.DR_VSERVER'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS default.DR_VSERVER (asup_id string,dt date)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'drop table asuprep.application_json_tab'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS asuprep.application_json_tab (application_name string, asup_id string,dt date)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "\n",
    "que = 'drop table application_json_tab'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS application_json_tab (application_name string, asup_id string,dt date)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "\n",
    "que = 'drop table default.bart_diskerrors_n'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS default.bart_diskerrors_n (asup_id string,dt date)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'drop table test_nb_db.IB_MASTER_RAW'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = '''CREATE table IF NOT EXISTS test_nb_db.IB_MASTER_RAW  \n",
    "(SYS_SERIAL_NO  int, IOBJECTID_SAP int, SYSTEMID_SAP int, HOSTNAME_SAP string, \n",
    "SYS_PRODID_SAP string, SYS_VERSION string, SYS_SHIPDATE_SAP DATE, FIRST_ASUP_DAY date, \n",
    "LAST_ASUP_DAY date, CUSTOMERID_SAP string, SITEID_SAP string, CUST_ACCTYPE_SAP string,\n",
    "CUST_NAME_SAP string, CUST_NAGP string, CUST_NAME string, CUST_SITENAME_SAP string, \n",
    "CUST_DESCR_SAP string, SYS_STATUS_SAP string, SYS_ASUPSTATUS_SAP string, MKT_CLASSIF_SAP string,\n",
    "MKT_SEGMENT_SAP string, NA_CMAT_ID_SAP string, PARENTID_SAP string, AUTHORIZATIONGRP_SAP string,\n",
    "ETL_DATE date, SYS_MODEL string, SYSTEM_NAME string, SYS_MODE string, SALES_ORDER_NUMBER string,\n",
    "SALES_CHANNEL_TYPE string, SHIPPED_DATE date, INSTALLED_DATE date, MANUAL_INSTALLED_DATE_FLAG date,\n",
    "LAST_ASUP_DATE date, ASUP_STATUS date, CUST_E1000 int, CUST_SITE string, CUST_CITY string, \n",
    "CUST_STATE string, CUST_COUNTRY string, CUST_ACC_CLASSIFICATION string, \n",
    "INSTALLED_BASE_GROUP string, INSTALLED_PROD_STATUS string, SERIAL_NO_OWNER_NAGP string,\n",
    "SERIAL_NO_OWNER_NAME string, TGA_FLAG string, HA_PAIR_FLAG string, CUST_RESELLER_PARTY_NAME string,\n",
    "CUST_ASP_NAME string, CUST_SP_NAME string, CUST_TPM_NAME string, CUST_SALES_GEO string, \n",
    "CUST_SERVICE_GEO string, CUST_SERVICE_AREA string, SYS_NETAPP string,\n",
    "SYS_NAME string, ID1 string, SERVICE_LEVEL string,  SYS_START_DATE string, \n",
    "SYS_END_DATE string, CUST_SEA string, CUST_SEA_GEO string, CUST_SAM string, \n",
    "TOP_CUST_NAME string, CUST_G50 string, MISSING_FLAG string, CUST_G100 string, \n",
    "SYS_ASP string, SALES_DISTRICT string, CUST_NAGP_OLD string, CUST_NAME_OLD string, \n",
    "SYS_FAMILY string, SYS_TOPFAMILY string, SYS_FAMILYNAME string, \n",
    "SYS_PLATFORM_RANGE string, SYS_TYPE string, CUST_G100_OLD  string)'''\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'drop table test_nb_db.ib_master_s'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = '''CREATE table IF NOT EXISTS test_nb_db.ib_master_s\n",
    "(cust_name_flg int)'''\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'drop table guestdb.A_Adhoc_sysconf_a_sfp'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS guestdb.A_Adhoc_sysconf_a_sfp (asup_id string,text string)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'drop table guestdb.A_Adhoc_sysconf_b_sfp'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS guestdb.A_Adhoc_sysconf_b_sfp (asup_id string,text string)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'drop table asuprep.A_Adhoc_SFP_INC11177138'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS asuprep.A_Adhoc_SFP_INC11177138 (asup_id string,text string)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'drop table default.ES_ARS'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS default.ES_ARS (asup_id string,dt date)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'drop table bart_diskerrors'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS bart_diskerrors (asup_id string,dt date, serial_number string)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'drop table onecollect_header'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS onecollect_header (asup_id string, user_id string, hostname string, dt date)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'drop table default.DFM_PERFORMANCE_OBJECT'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS default.DFM_PERFORMANCE_OBJECT (asup_id string,dt date)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    " \n",
    "que = 'drop table processed_cm_stats_weekly'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS processed_cm_stats_weekly (asup_id string,dt date)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "\n",
    "que = 'drop table shelf_table'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = '''CREATE table IF NOT EXISTS shelf_table (asup_id string, shelf_serial_no string, shelf_path_name string,  \n",
    "shelf_type string, shelf_id_front_panel string, asup_key string, dt date)'''\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'drop table bartdb.bart_diskerrors_n '\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = '''CREATE table IF NOT EXISTS bartdb.bart_diskerrors_n (diskname string, sys_serial_no string, asup_id string) '''\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'drop table bartdb.bart_hagroupdrivepop'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = '''CREATE table IF NOT EXISTS bartdb.bart_hagroupdrivepop (sd_asup_id string, \n",
    "sys_serial_no string, sd_diskid string, sd_drivemodel string, sd_firmware string, sd_serial string,\n",
    "sd_power_on_hours int, sd_blocks_read int, sd_blocks_written int, \n",
    "sd_glist_count int, sd_rated_life_used int, sd_spare_blocks_consumed int) '''\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'drop table bartdb.bart_disk_errors_f'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = '''CREATE table IF NOT EXISTS bartdb.bart_disk_errors_f\n",
    "(diskname string, sys_serial_no string,\n",
    "sd_asup_id string, sd_sys_serial string, sd_diskid string, \n",
    "sd_drivemodel string, sd_firmware string, sd_serial string,\n",
    "sd_power_on_hours int, sd_blocks_read int, sd_blocks_written int, \n",
    "sd_glist_count int, sd_rated_life_used int, sd_spare_blocks_consumed int, max_asup_id string)'''\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'drop table cm_stats_weekly'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = '''CREATE table IF NOT EXISTS cm_stats_weekly\n",
    "(object_name string, asup_key string, counters map<string,int>,\n",
    "cur_time_stamp date, dt date, instance_name string)'''\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'drop table consumption.oci_svc_consumption_list_details'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = '''CREATE table IF NOT EXISTS consumption.oci_svc_consumption_list_details\n",
    "(consumed_qt string, calc_key string, service_nm string, weekend_date date, \n",
    "sys_serial_no string, dt date, consumption_level string)\n",
    "'''\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'drop table ebadb.lsg_max_date'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = '''CREATE table IF NOT EXISTS ebadb.lsg_max_date\n",
    "(table_name string, dt date, max_date date, min_date date, load_date date) '''\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "\n",
    "que = 'drop table ebadb.es_drive_LSG'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = '''create table if not exists  ebadb.es_drive_LSG (asup_id string, Volume_Group_Index int, \n",
    "Drive_media_type string, Raw_capacity int, Usable_Capacity int, Serial_number string, sys_serial_no string,\n",
    "Available string, Hotspare string, Drive_offline string, Removed string, Manufacturer string,\n",
    "Productid string, dt date, max_asup_id string,max_dt date, serial_no string)'''\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'drop table default.es_drive'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = '''CREATE table IF NOT EXISTS default.es_drive (asup_id string, Available string,\n",
    "Hotspare string, Drive_offline string, Removed string, Manufacturer string, Productid string, dt date,\n",
    "sys_serial_no string, serialnumber string, usablecapacity int, rawcapacity int, drivemediatype string,\n",
    "volumegroupindex string)'''\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'drop table ebadb.es_free_extent_LSG'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = '''CREATE table IF NOT EXISTS ebadb.es_free_extent_LSG (asup_id string, asup_key string, system_key string,\n",
    "system_id string, sys_serial_no string, asup_gen_date date, rawcapacity int, volumegroupref string, dt date)'''\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "\n",
    "que = 'drop table default.es_free_extent'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = '''CREATE table IF NOT EXISTS default.es_free_extent (asup_id string, asup_key string, system_key string,\n",
    "system_id string, sys_serial_no string, asup_gen_date date, rawcapacity int, volumegroupref string, dt date)'''\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'drop table ebadb.lsg_es_free_extent_sn'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS ebadb.lsg_es_free_extent_sn (sys_serial_no string,max_dt date, max_asup_id string)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'drop table ebadb.lsg_es_drive_sn'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS ebadb.lsg_es_drive_sn (sys_serial_no string,max_dt date, max_asup_id string)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'drop table ebadb.lsg_es_drive_sn'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS ebadb.lsg_es_drive_sn (sys_serial_no string,max_dt date, max_asup_id string)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'drop table default.es_system'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = '''CREATE table IF NOT EXISTS default.es_system (\n",
    "Hostname string,asup_id string, asup_gen_date date, sys_model string, sys_serial_no string, \n",
    "sys_type string, sys_version string, dt date)'''\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'drop table ebadb.lsg_es_system_sn'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS ebadb.lsg_es_system_sn (sys_serial_no string,max_dt date, max_asup_id string)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'drop table ebadb.es_volume_LSG'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = '''CREATE table IF NOT EXISTS ebadb.es_volume_LSG \n",
    "(sys_serial_no string, asup_id string, label string, capacity int, \n",
    "volume_offline string, worldwidename string, dt date, volumegroupref string,\n",
    "max_asup_id string,max_dt date,sys_serial_n string)'''\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'drop table default.es_volume'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = '''CREATE table IF NOT EXISTS default.es_volume \n",
    "(sys_serial_no string, asup_id string, label string, capacity int, \n",
    "volume_offline string, worldwidename string, dt date, volumegroupref string)'''\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "INSERT OVERWRITE TABLE ebadb.lsg_es_volume_sn \n",
    "SELECT \n",
    "COALESCE(b.dt,a.max_dt) as dt,\n",
    "COALESCE(b.asup_id,a.max_asup_id) as asup_id,\n",
    "COALESCE(b.sys_serial_no,a.sys_serial_no) as sys_serial_no\n",
    "FROM ebadb.lsg_es_volume_sn a\n",
    "left outer join (\n",
    "select max(dt) as dt,max(asup_id) as asup_id,sys_serial_no\n",
    "from ebadb.es_volume_LSG\n",
    "group by sys_serial_no) b \n",
    "on a.sys_serial_no = b.sys_serial_no \n",
    "\n",
    "que = 'drop table ebadb.lsg_es_volume_sn'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = '''CREATE table IF NOT EXISTS ebadb.lsg_es_volume_sn \n",
    "(sys_serial_no string, asup_id string, max_asup_id string, label string, capacity int, \n",
    "max_dt date, volume_offline string, worldwidename string, dt date)'''\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'drop table default.es_system'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = '''CREATE table IF NOT EXISTS default.es_system (\n",
    "Hostname string,asup_id string, asup_gen_date date, sys_model string, sys_serial_no string, \n",
    "sys_type string, sys_version string, dt date)'''\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'drop table ebadb.lsg_es_system_sn'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = '''CREATE table IF NOT EXISTS ebadb.lsg_es_system_sn (sys_serial_no string,max_dt date, max_asup_id string)'''\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'drop table ebadb.lsg_es_volume_group_sn'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = '''CREATE table IF NOT EXISTS ebadb.lsg_es_volume_group_sn (\n",
    "sys_serial_no string, max_asup_id string, max_dt date)'''\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'drop table default.es_volume_group'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = '''CREATE table IF NOT EXISTS default.es_volume_group \n",
    "(sys_serial_no string, asup_id string, asup_gen_date date,\n",
    "volumegroupref string, label string, dt date, serial_no string,\n",
    "max_asup_id string, max_dt date)'''\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'drop table bartdb.bart_diskerrors'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = '''CREATE table IF NOT EXISTS bartdb.bart_diskerrors \n",
    "(asup_id string, diskname string, Sys_serial_no string, \n",
    "op string, sector int, senseInfo string, sCode string, \n",
    "sense_key string, sense_code string, qualifier string, \n",
    "fru_failed string, reason string, powerontime string, \n",
    "Error string, ErrorTime string, sys_version string, \n",
    "Drive_Model string, FMV string, Serial_Number string, event_type string)'''\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'drop table default.bart_diskerrors_m'\n",
    "conn.execute(que, async=False)\n",
    "        \n",
    "que = '''CREATE table IF NOT EXISTS default.bart_diskerrors_m\n",
    "(asup_id string, diskname string, Sys_serial_no string, \n",
    "op string, sector int, senseInfo string, sCode string, \n",
    "sense_key string, sense_code string, qualifier string, \n",
    "fru_failed string, reason string, powerontime string, \n",
    "Error string, ErrorTime string, sys_version string, \n",
    "Drive_Model string, FMV string, Serial_Number string, event_type string)'''\n",
    "conn.execute(que, async=False)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "774 \n",
      "\n",
      "\n",
      "\n",
      "INSERT OVERWRITE TABLE ebadb.lsg_es_volume_sn \n",
      "SELECT \n",
      "COALESCE(b.dt,a.max_dt) as dt,\n",
      "COALESCE(b.asup_id,a.max_asup_id) as asup_id,\n",
      "COALESCE(b.sys_serial_no,a.sys_serial_no) as sys_serial_no\n",
      "FROM ebadb.lsg_es_volume_sn a\n",
      "left outer join (\n",
      "select max(dt) as dt,max(asup_id) as asup_id,sys_serial_no\n",
      "from ebadb.es_volume_LSG\n",
      "group by sys_serial_no) b \n",
      "on a.sys_serial_no = b.sys_serial_no \n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "Operational Error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-289-68840ac65f3f>\u001b[0m in \u001b[0;36moperator_footprint\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"explain FORMATTED \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m             \u001b[0mexec_plan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetchall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyhive/hive.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, operation, parameters, **kwargs)\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExecuteStatement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m         \u001b[0m_check_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_operationHandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moperationHandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyhive/hive.py\u001b[0m in \u001b[0;36m_check_status\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatusCode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mttypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTStatusCode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSUCCESS_STATUS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mOperationalError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=[\"*org.apache.hive.service.cli.HiveSQLException:Error while compiling statement: FAILED: SemanticException [Error 10044]: Line 5:23 Cannot insert into target table because column number/types are different 'lsg_es_volume_sn': Table insclause-0 has 10 columns, but query has 3 columns.:28:27\", 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:316', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:112', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:181', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:257', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:388', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatement:HiveSessionImpl.java:369', 'sun.reflect.GeneratedMethodAccessor52:invoke::-1', 'sun.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:498', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:-2', 'javax.security.auth.Subject:doAs:Subject.java:422', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1844', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'com.sun.proxy.$Proxy20:executeStatement::-1', 'org.apache.hive.service.cli.CLIService:executeStatement:CLIService.java:262', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:490', 'org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1313', 'org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1298', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:39', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:39', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:285', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1149', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:624', 'java.lang.Thread:run:Thread.java:748', \"*org.apache.hadoop.hive.ql.parse.SemanticException:Line 5:23 Cannot insert into target table because column number/types are different 'lsg_es_volume_sn': Table insclause-0 has 10 columns, but query has 3 columns.:44:17\", 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:genConversionSelectOperator:SemanticAnalyzer.java:6774', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:genFileSinkPlan:SemanticAnalyzer.java:6569', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:genPostGroupByBodyPlan:SemanticAnalyzer.java:9020', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:genBodyPlan:SemanticAnalyzer.java:8911', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:genPlan:SemanticAnalyzer.java:9756', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:genPlan:SemanticAnalyzer.java:9649', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:genOPTree:SemanticAnalyzer.java:10122', 'org.apache.hadoop.hive.ql.parse.CalcitePlanner:genOPTree:CalcitePlanner.java:326', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:analyzeInternal:SemanticAnalyzer.java:10133', 'org.apache.hadoop.hive.ql.parse.CalcitePlanner:analyzeInternal:CalcitePlanner.java:210', 'org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer:analyze:BaseSemanticAnalyzer.java:227', 'org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer:analyzeInternal:ExplainSemanticAnalyzer.java:74', 'org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer:analyze:BaseSemanticAnalyzer.java:227', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:425', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:309', 'org.apache.hadoop.hive.ql.Driver:compileInternal:Driver.java:1145', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1139', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:110'], sqlState='42000', errorCode=10044, errorMessage=\"Error while compiling statement: FAILED: SemanticException [Error 10044]: Line 5:23 Cannot insert into target table because column number/types are different 'lsg_es_volume_sn': Table insclause-0 has 10 columns, but query has 3 columns.\"), operationHandle=None)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-289-68840ac65f3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m \u001b[0mdt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'operators'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'query'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperator_footprint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   3846\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3847\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3848\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3850\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-289-68840ac65f3f>\u001b[0m in \u001b[0;36moperator_footprint\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mNameError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Operational Error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;31m#if i > 1000:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;31m#    raise NameError(\"Stop\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: Operational Error"
     ]
    }
   ],
   "source": [
    "from pyhive import hive\n",
    "import ast\n",
    "\n",
    "def process_tree(tree):\n",
    "    if not tree: return ''\n",
    "    for op in tree:\n",
    "        if 'children' not in tree[op]: \n",
    "            return op.replace(' Operator', '')\n",
    "        return op.replace(' Operator', '') + ' | ' + process_tree(tree[op]['children'])\n",
    "        \n",
    "\n",
    "def process_execution_plan(exp):\n",
    "    operations = {}\n",
    "    for stage_type in exp:\n",
    "        if stage_type == 'Map Reduce':\n",
    "            map_trees = exp[stage_type]['Map Operator Tree:']\n",
    "            reduce_tree = exp[stage_type]['Reduce Operator Tree:']\n",
    "            \n",
    "            for tree in map_trees:\n",
    "                operations['map'] = process_tree(tree)\n",
    "            operations['reduce'] = process_tree(reduce_tree)\n",
    "    return operations\n",
    "\n",
    "def operator_footprint(query):\n",
    "    global i    \n",
    "    i = i + 1\n",
    "    \n",
    "    if i < 760:\n",
    "        return\n",
    "    \n",
    "    if 'dt = 2018' in query:\n",
    "        print(i, 'return')\n",
    "        return;\n",
    "    \n",
    "    if 'create table AFF_VOLUME as' in query:\n",
    "        que = 'drop table AFF_VOLUME'\n",
    "        conn.execute(que, async=False)\n",
    "        \n",
    "        try:\n",
    "            conn.execute(\"explain FORMATTED \" + query, async=False)\n",
    "            exec_plan = conn.fetchall()\n",
    "        except:\n",
    "            print(i, query, '\\n')\n",
    "            raise NameError(\"Operational Error\")\n",
    "        \n",
    "        que = '''CREATE table IF NOT EXISTS AFF_VOLUME (asup_id string,sys_serial_no string,\n",
    "        sys_version string,aggr_id string,vol_name string, vol_state_asis string, \n",
    "        vol_compressed_kb int, guarantee  string, vol_is_vsroot  string, vol_is_root string,\n",
    "        vol_is_online string,vol_asis_saved_kb int, vol_asis_saved_pct int, \n",
    "        vol_compressed_pct int, Aggregate_Used_KB int, Vol_size int)'''\n",
    "        conn.execute(que, async=False)\n",
    "        \n",
    "    elif \"create table OntapAdoptionDevice as\" in query:\n",
    "        que = 'drop table OntapAdoptionDevice'\n",
    "        conn.execute(que, async=False)\n",
    "        \n",
    "        try:\n",
    "            conn.execute(\"explain FORMATTED \" + query, async=False)\n",
    "            exec_plan = conn.fetchall()\n",
    "        except:\n",
    "            print(i, query, '\\n')\n",
    "            raise NameError(\"Operational Error\")\n",
    "        \n",
    "        que = 'CREATE table IF NOT EXISTS OntapAdoptionDevice (asup_key string, asup_id string, dt date, dvc_disk_type string, dvc_serial_no string, dvc_phy_size_mb string, dvc_type string, dvc_label string)'\n",
    "        conn.execute(que, async=False)      \n",
    "    elif 'create table guestdb.puat_nsm_ocsm_data AS' in query:\n",
    "        que = 'drop table guestdb.puat_nsm_ocsm_data'\n",
    "        conn.execute(que, async=False)\n",
    "\n",
    "        try:\n",
    "            conn.execute(\"explain FORMATTED \" + query, async=False)\n",
    "            exec_plan = conn.fetchall()\n",
    "        except:\n",
    "            print(i, query, '\\n')\n",
    "            raise NameError(\"Operational Error\")\n",
    "        \n",
    "        que = 'CREATE table IF NOT EXISTS guestdb.puat_nsm_ocsm_data (asup_id string, counterkey string, countervalue string, dt date)'\n",
    "        conn.execute(que, async=False)\n",
    "        \n",
    "    elif 'create table guestdb.puat_sm_counter_final as' in query:\n",
    "        que = 'drop table guestdb.puat_sm_counter_final'\n",
    "        conn.execute(que, async=False)\n",
    "        \n",
    "        try:\n",
    "            conn.execute(\"explain FORMATTED \" + query, async=False)\n",
    "            exec_plan = conn.fetchall()\n",
    "        except:\n",
    "            print(i, query, '\\n')\n",
    "            raise NameError(\"Operational Error\")\n",
    "        \n",
    "        que = '''CREATE table IF NOT EXISTS\n",
    "        guestdb.puat_sm_counter_final\n",
    "        (Asup_Month string, asup_id string,\n",
    "        Sys_serial_No string,\n",
    "        Sys_Version string, Sys_Version_Fbranch string, \n",
    "        Sys_operating_Mode  string, Sys_Model string,\n",
    "        Customer_Name string, Parent_Name string,\n",
    "        site_country_name string, Service_geo string,\n",
    "        g50_customer_flag string, a50_customer_flag string,\n",
    "        e1000_customer_flag string, roe_customer_flag string,\n",
    "        not_e1000_customer_flag string, msb_customer_flag string,\n",
    "        counterkey string, countervalue string)'''\n",
    "        conn.execute(que, async=False)\n",
    "        \n",
    "    elif 'create table if not exists ebadb.es_drive_LSG as' in query:\n",
    "        que = 'drop table ebadb.es_drive_LSG'\n",
    "        conn.execute(que, async=False)\n",
    "        \n",
    "        try:\n",
    "            conn.execute(\"explain FORMATTED \" + query, async=False)\n",
    "            exec_plan = conn.fetchall()\n",
    "        except:\n",
    "            print(i, query, '\\n')\n",
    "            raise NameError(\"Operational Error\")\n",
    "        \n",
    "        que = '''create table if not exists  ebadb.es_drive_LSG (asup_id string, Volume_Group_Index int, \n",
    "        Drive_media_type string, Raw_capacity int, Usable_Capacity int, Serial_number string, sys_serial_no string, \n",
    "        Avalable string, Hotspare string, Drive_offline string, Removed string, Manufacturer string,\n",
    "        Productid string, dt date, max_asup_id string,max_dt date, serial_no string)'''\n",
    "        conn.execute(que, async=False)\n",
    "\n",
    "    elif 'create table if not exists ebadb.es_system_LSG' in query:\n",
    "        que = 'drop table ebadb.es_system_LSG'\n",
    "        conn.execute(que, async=False)\n",
    "        \n",
    "        try:\n",
    "            conn.execute(\"explain FORMATTED \" + query, async=False)\n",
    "            exec_plan = conn.fetchall()\n",
    "        except:\n",
    "            print(i, query, '\\n')\n",
    "            raise NameError(\"Operational Error\")\n",
    "        \n",
    "        que = '''create table if not exists ebadb.es_system_LSG (\n",
    "        Hostname string,asup_id string, asup_gen_date date, sys_model string, sys_serial_no string, \n",
    "        sys_type string, sys_version string, dt date)'''\n",
    "        conn.execute(que, async=False)\n",
    "        \n",
    "    elif 'create table if not exists ebadb.cm_LSG as' in query:\n",
    "        que = 'drop table ebadb.cm_LSG'\n",
    "        conn.execute(que, async=False)\n",
    "\n",
    "        try:\n",
    "            conn.execute(\"explain FORMATTED \" + query, async=False)\n",
    "            exec_plan = conn.fetchall()\n",
    "        except:\n",
    "            print(i, query, '\\n')\n",
    "            raise NameError(\"Operational Error\")\n",
    "        \n",
    "        \n",
    "        que = '''CREATE table IF NOT EXISTS ebadb.cm_LSG (\n",
    "        hostname string, sys_domain string, asup_id string,\n",
    "        asup_key string, DiskCapacity_MB int, SN string, instance_name string,\n",
    "        cur_time_stamp date, time int, dt date)'''\n",
    "        conn.execute(que, async=False)\n",
    "        \n",
    "    elif 'CREATE table IF NOT EXISTS ebadb.es_volume_group_LSG' in query:\n",
    "        que = 'drop table ebadb.es_volume_group_LSG'\n",
    "        conn.execute(que, async=False)\n",
    "        \n",
    "        try:\n",
    "            conn.execute(\"explain FORMATTED \" + query, async=False)\n",
    "            exec_plan = conn.fetchall()\n",
    "        except:\n",
    "            print(i, query, '\\n')\n",
    "            raise NameError(\"Operational Error\")\n",
    "        \n",
    "        que = '''CREATE table IF NOT EXISTS ebadb.es_volume_group_LSG\n",
    "        (sys_serial_no string, asup_id string, asup_gen_date date,\n",
    "        volumegroupref string, label string, dt date)'''\n",
    "        conn.execute(que, async=False)\n",
    "    elif 'Create Table guestdb.A_Adhoc_SFP_INC11177138 As' in query:\n",
    "        que = 'drop table guestdb.A_Adhoc_SFP_INC11177138'\n",
    "        conn.execute(que, async=False)\n",
    "        \n",
    "        try:\n",
    "            conn.execute(\"explain FORMATTED \" + query, async=False)\n",
    "            exec_plan = conn.fetchall()\n",
    "        except:\n",
    "            print(i, query, '\\n')\n",
    "            raise NameError(\"Operational Error\") \n",
    "            \n",
    "        que = '''CREATE table IF NOT EXISTS guestdb.A_Adhoc_SFP_INC11177138\n",
    "        (asup_id string, Slot_No string, port_no int, Mac_address string, \n",
    "        SFP_Vendor string, SFP_PartNumber string, SFP_SerialNumber string)'''\n",
    "        conn.execute(que, async=False)\n",
    "        \n",
    "    elif 'Create Table guestdb.A_Adhoc_SFP_INC11177138_1 As' in query:\n",
    "        que = 'drop table guestdb.A_Adhoc_SFP_INC11177138_1'\n",
    "        conn.execute(que, async=False)\n",
    "        \n",
    "        try:\n",
    "            conn.execute(\"explain FORMATTED \" + query, async=False)\n",
    "            exec_plan = conn.fetchall()\n",
    "        except:\n",
    "            print(i, query, '\\n')\n",
    "            raise NameError(\"Operational Error\") \n",
    "        \n",
    "        que = '''CREATE table IF NOT EXISTS guestdb.A_Adhoc_SFP_INC11177138_1\n",
    "        (asup_id string, Slot_No string, port_no int, Mac_address string, \n",
    "        SFP_Vendor string, SFP_PartNumber string, SFP_SerialNumber string)'''\n",
    "        conn.execute(que, async=False)\n",
    "        \n",
    "    elif 'create table bartdb.bart_diskerrors_m' in query:\n",
    "        que = 'drop table bartdb.bart_diskerrors_m'\n",
    "        conn.execute(que, async=False)\n",
    "        \n",
    "        try:\n",
    "            conn.execute(\"explain FORMATTED \" + query, async=False)\n",
    "            exec_plan = conn.fetchall()\n",
    "        except:\n",
    "            print(i, query, '\\n')\n",
    "            raise NameError(\"Operational Error\") \n",
    "        \n",
    "        que = '''CREATE table IF NOT EXISTS bartdb.bart_diskerrors_m\n",
    "        (asup_id string, diskname string, Sys_serial_no string, \n",
    "        op string, sector int, senseInfo string, sCode string, \n",
    "        sense_key string, sense_code string, qualifier string, \n",
    "        fru_failed string, reason string, powerontime string, \n",
    "        Error string, ErrorTime string, sys_version string, \n",
    "        Drive_Model string, FMV string, Serial_Number string, event_type string)'''\n",
    "        conn.execute(que, async=False)\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        try:\n",
    "            conn.execute(\"explain FORMATTED \" + query, async=False)\n",
    "            exec_plan = conn.fetchall()\n",
    "        except:\n",
    "            print(i, query, '\\n')\n",
    "            raise NameError(\"Operational Error\")\n",
    "    #if i > 1000:\n",
    "    #    raise NameError(\"Stop\")\n",
    "    #print(exec_plan)\n",
    "    #stages = {}\n",
    "    #for stgid in exec_plan['STAGE PLANS']:\n",
    "    #    operations = process_execution_plan(exec_plan['STAGE PLANS'][stgid])\n",
    "    #    if operations:\n",
    "    #        stages[stgid] = operations\n",
    "    \n",
    "    #print(json.dumps(stages, indent=4))\n",
    "    #return stages\n",
    "\n",
    "i=0;\n",
    "dt['operators'] = dt['query'].apply(operator_footprint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=['*org.apache.hive.service.cli.HiveSQLException:Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Database consumption already exists:28:27', 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:316', 'org.apache.hive.service.cli.operation.SQLOperation:runQuery:SQLOperation.java:156', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:183', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:257', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:388', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatement:HiveSessionImpl.java:369', 'sun.reflect.GeneratedMethodAccessor52:invoke::-1', 'sun.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:498', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:-2', 'javax.security.auth.Subject:doAs:Subject.java:422', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1844', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'com.sun.proxy.$Proxy20:executeStatement::-1', 'org.apache.hive.service.cli.CLIService:executeStatement:CLIService.java:262', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:490', 'org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1313', 'org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1298', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:39', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:39', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:285', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1149', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:624', 'java.lang.Thread:run:Thread.java:748', '*org.apache.hadoop.hive.ql.metadata.HiveException:Database consumption already exists:36:9', 'org.apache.hadoop.hive.ql.exec.DDLTask:createDatabase:DDLTask.java:3899', 'org.apache.hadoop.hive.ql.exec.DDLTask:execute:DDLTask.java:271', 'org.apache.hadoop.hive.ql.exec.Task:executeTask:Task.java:160', 'org.apache.hadoop.hive.ql.exec.TaskRunner:runSequential:TaskRunner.java:88', 'org.apache.hadoop.hive.ql.Driver:launchTask:Driver.java:1676', 'org.apache.hadoop.hive.ql.Driver:execute:Driver.java:1435', 'org.apache.hadoop.hive.ql.Driver:runInternal:Driver.java:1218', 'org.apache.hadoop.hive.ql.Driver:run:Driver.java:1082', 'org.apache.hadoop.hive.ql.Driver:run:Driver.java:1077', 'org.apache.hive.service.cli.operation.SQLOperation:runQuery:SQLOperation.java:154', '*org.apache.hadoop.hive.metastore.api.AlreadyExistsException:Database consumption already exists:51:15', 'org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler:create_database:HiveMetaStore.java:891', 'sun.reflect.NativeMethodAccessorImpl:invoke0:NativeMethodAccessorImpl.java:-2', 'sun.reflect.NativeMethodAccessorImpl:invoke:NativeMethodAccessorImpl.java:62', 'sun.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:498', 'org.apache.hadoop.hive.metastore.RetryingHMSHandler:invoke:RetryingHMSHandler.java:107', 'com.sun.proxy.$Proxy8:create_database::-1', 'org.apache.hadoop.hive.metastore.HiveMetaStoreClient:createDatabase:HiveMetaStoreClient.java:645', 'sun.reflect.NativeMethodAccessorImpl:invoke0:NativeMethodAccessorImpl.java:-2', 'sun.reflect.NativeMethodAccessorImpl:invoke:NativeMethodAccessorImpl.java:62', 'sun.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:498', 'org.apache.hadoop.hive.metastore.RetryingMetaStoreClient:invoke:RetryingMetaStoreClient.java:156', 'com.sun.proxy.$Proxy9:createDatabase::-1', 'org.apache.hadoop.hive.ql.metadata.Hive:createDatabase:Hive.java:306', 'org.apache.hadoop.hive.ql.exec.DDLTask:createDatabase:DDLTask.java:3895'], sqlState='08S01', errorCode=1, errorMessage='Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Database consumption already exists'), operationHandle=None)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-209-ef587aa2b46f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mque\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'CREATE database consumption'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mque\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyhive/hive.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, operation, parameters, **kwargs)\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExecuteStatement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m         \u001b[0m_check_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_operationHandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moperationHandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyhive/hive.py\u001b[0m in \u001b[0;36m_check_status\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatusCode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mttypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTStatusCode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSUCCESS_STATUS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mOperationalError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=['*org.apache.hive.service.cli.HiveSQLException:Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Database consumption already exists:28:27', 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:316', 'org.apache.hive.service.cli.operation.SQLOperation:runQuery:SQLOperation.java:156', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:183', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:257', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:388', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatement:HiveSessionImpl.java:369', 'sun.reflect.GeneratedMethodAccessor52:invoke::-1', 'sun.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:498', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:-2', 'javax.security.auth.Subject:doAs:Subject.java:422', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1844', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'com.sun.proxy.$Proxy20:executeStatement::-1', 'org.apache.hive.service.cli.CLIService:executeStatement:CLIService.java:262', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:490', 'org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1313', 'org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1298', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:39', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:39', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:285', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1149', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:624', 'java.lang.Thread:run:Thread.java:748', '*org.apache.hadoop.hive.ql.metadata.HiveException:Database consumption already exists:36:9', 'org.apache.hadoop.hive.ql.exec.DDLTask:createDatabase:DDLTask.java:3899', 'org.apache.hadoop.hive.ql.exec.DDLTask:execute:DDLTask.java:271', 'org.apache.hadoop.hive.ql.exec.Task:executeTask:Task.java:160', 'org.apache.hadoop.hive.ql.exec.TaskRunner:runSequential:TaskRunner.java:88', 'org.apache.hadoop.hive.ql.Driver:launchTask:Driver.java:1676', 'org.apache.hadoop.hive.ql.Driver:execute:Driver.java:1435', 'org.apache.hadoop.hive.ql.Driver:runInternal:Driver.java:1218', 'org.apache.hadoop.hive.ql.Driver:run:Driver.java:1082', 'org.apache.hadoop.hive.ql.Driver:run:Driver.java:1077', 'org.apache.hive.service.cli.operation.SQLOperation:runQuery:SQLOperation.java:154', '*org.apache.hadoop.hive.metastore.api.AlreadyExistsException:Database consumption already exists:51:15', 'org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler:create_database:HiveMetaStore.java:891', 'sun.reflect.NativeMethodAccessorImpl:invoke0:NativeMethodAccessorImpl.java:-2', 'sun.reflect.NativeMethodAccessorImpl:invoke:NativeMethodAccessorImpl.java:62', 'sun.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:498', 'org.apache.hadoop.hive.metastore.RetryingHMSHandler:invoke:RetryingHMSHandler.java:107', 'com.sun.proxy.$Proxy8:create_database::-1', 'org.apache.hadoop.hive.metastore.HiveMetaStoreClient:createDatabase:HiveMetaStoreClient.java:645', 'sun.reflect.NativeMethodAccessorImpl:invoke0:NativeMethodAccessorImpl.java:-2', 'sun.reflect.NativeMethodAccessorImpl:invoke:NativeMethodAccessorImpl.java:62', 'sun.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:498', 'org.apache.hadoop.hive.metastore.RetryingMetaStoreClient:invoke:RetryingMetaStoreClient.java:156', 'com.sun.proxy.$Proxy9:createDatabase::-1', 'org.apache.hadoop.hive.ql.metadata.Hive:createDatabase:Hive.java:306', 'org.apache.hadoop.hive.ql.exec.DDLTask:createDatabase:DDLTask.java:3895'], sqlState='08S01', errorCode=1, errorMessage='Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Database consumption already exists'), operationHandle=None)"
     ]
    }
   ],
   "source": [
    "que = 'CREATE database ebadb'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE database consumption'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'CREATE database bartdb'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE database test_nb_db'\n",
    "conn.execute(que, async=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "que = 'CREATE table IF NOT EXISTS UBS_SHELF_ASUPS_SD (dt date, asup_id string)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS default.hdfs_latest_calmonth (dt date, asup_id string)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS UBS_SHELF_ASUPS_1 (dt date, asup_id string)'\n",
    "conn.execute(que, async=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "insert into asuprep.summary_attributes\n",
    "select a.asup_id,\n",
    "oid,\n",
    "path,\n",
    "attr,\n",
    "name,\n",
    "value,\n",
    "index,\n",
    "tableid,\n",
    "tablename,\n",
    "dt from asuprep.summary_attributes_temp a\n",
    "where a.asup_id not in (select distinct asup_id from asuprep.summary_attributes)\n",
    "'''\n",
    "\n",
    "\n",
    "que = 'CREATE database asuprep'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS asuprep.summary_attributes (asup_id string, oid string, path string, attr string, name string, value string, index string, tableid string, tablename string, dt date)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS asuprep.summary_attributes_temp (asup_id string, oid string, path string, attr string, name string, value string, index string, tableid string, tablename string, dt date)'\n",
    "conn.execute(que, async=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "create table OntapAdoptionDevice as\n",
    "select a.* from default.device a,(\n",
    "select split(asup_key,'\\\\|')[3] serial,max(asup_id) asup_id \n",
    "from default.device where dt>='20180601' and dt<='20180802'\n",
    "group by split(asup_key,'\\\\|')[3]) b\n",
    "where a.asup_id=b.asup_id \n",
    "'''\n",
    "\n",
    "'''\n",
    "create table OntapAdoptionDevice_final as\n",
    "select split(asup_key,'\\\\|')[3] serial,dvc_disk_type,count(distinct dvc_serial_no),SUM(dvc_phy_size_mb)\n",
    "from OntapAdoptionDevice where dvc_type IN ('DISK','LUN') and dvc_label<>'PARTNER'\n",
    "group by split(asup_key,'\\\\|')[3] ,dvc_disk_type \n",
    "'''\n",
    "\n",
    "que = 'drop table default.device'\n",
    "#conn.execute(que, async=False)\n",
    "\n",
    "que = 'drop table OntapAdoptionDevice'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS default.device (asup_key string, asup_id string, dt date, dvc_disk_type string, dvc_serial_no string, dvc_phy_size_mb string, dvc_type string, dvc_label string)'\n",
    "#conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS OntapAdoptionDevice (asup_key string, asup_id string, dt date, dvc_disk_type string, dvc_serial_no string, dvc_phy_size_mb string, dvc_type string, dvc_label string)'\n",
    "#conn.execute(que, async=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Insert into application_record_ocum \n",
    "select a.asup_id, \n",
    "  get_json_object(value,'$.id') as id, \n",
    "  get_json_object(value,'$.url') as url, \n",
    "  get_json_object(value,'$.version') as version, \n",
    "  get_json_object(value,'$.date_added') as date_added, \n",
    "  get_json_object(value,'$.platform') as platform, \n",
    "  get_json_object(value,'$.hostFQDN') as hostFQDN, \n",
    "  get_json_object(value,'$.lastCheckin') as lastCheckin,\n",
    "  get_json_object(value,'$.modified_from') as modified_from\n",
    "from smfdbv2.application_record a join (select max(asup_id) asup_id from application_record_ocum) b\n",
    "where UPPER(name) like '%UNIFIED%'\n",
    "and a.asup_id > b.asup_id \n",
    "'''\n",
    "\n",
    "que = 'CREATE database guestdb'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE database smfdbv2'\n",
    "#conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS smfdbv2.application_record (value string, asup_id string, dt date, name string)'\n",
    "#conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS application_record_ocum (id string, url string, version string, date_added date, platform string, hostFQDN string, lastCheckin date, modified_from string, asup_id string)'\n",
    "#conn.execute(que, async=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS asuprep.application_record_ocum (id string, url string, version string, date_added date, platform string, hostFQDN string, lastCheckin date, modified_from string, asup_id string)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS default.system (asup_subject string, sys_serial_no string, asup_id string, system_id string, partner_system_id string,Hostname string,sys_model string,sys_version string,sys_operating_mode string, additional_fields  map<string,string>)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'INSERT INTO TABLE default.cluster_table VALUES (\"1\", \"1111\", \"1111\")'\n",
    "conn.execute(que, async=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "create table ontapadoptionsystem as\n",
    "select a.asup_id asup_id,s.additional_fields[\"is_all_flash_optimized\"] Aff \n",
    "from default.asup a, default.system s where a.asup_id=s.asup_id and a.asup_type='DOT-REGULAR' \n",
    "and a.dt>='20180601' and a.dt<='20180802' and\n",
    "s.dt>='20180601' and s.dt<='20180802'\n",
    "'''\n",
    "\n",
    "\n",
    "que = 'drop table default.asup'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS default.asup (asup_id string, asup_gen_date date, dt date, additional_fields  map<string,string>)'\n",
    "conn.execute(que, async=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''create table ontapadoptionprotocol as\n",
    "select split(asup_key,'\\\\|')[3] serial,max(tags[\"cifsOps\"]) CIFS_OPS,\n",
    "max(tags[\"httpOps\"]) HTTP_OPS,max(tags[\"nfsOps\"]) NFS_OPS,\n",
    "max(tags[\"fcpOps\"]) FCP_OPS,\n",
    "max(tags[\"iscsiOps\"]) ISCSI_OPS\n",
    "from default.ems a,(\n",
    "select split(asup_key,'\\\\|')[3] serial,max(tags[\"asup_id\"]) asup_id\n",
    "from default.ems where dt>='2018-06-01' and dt<='2018-08-02' \n",
    "and event_type='kern_uptime_filer_1' and (tags[\"cifsOps\"] <>0 or tags[\"cifsOps\"] IS NOT NULL or tags[\"fcpOps\"] <>0 or tags[\"fcpOps\"] IS NOT NULL or tags[\"nfsOps\"] <>0 or tags[\"nfsOps\"] IS NOT NULL \n",
    "    or tags[\"iscsiOps\"] <>0 or tags[\"iscsiOps\"] IS NOT NULL or tags[\"httpOps\"] <>0 or tags[\"httpOps\"] IS NOT NULL)\n",
    "group by split(asup_key,'\\\\|')[3]) b\n",
    "where a.tags[\"asup_id\"]=b.asup_id\n",
    "and a.dt>='2018-06-01' and a.dt<='2018-08-02'\n",
    "group by split(asup_key,'\\\\|')[3] '''\n",
    "\n",
    "'''select s.sys_serial_no,s.system_id,s.sys_model,s.sys_version,mb.mb_serial_no,mb.mb_partno,mb.asup_id,ems.dt,ems.tags\n",
    "from EMS ems,motherboard mb,system s\n",
    "where ems.dt between '2018-05-01' and '2018-08-02'\n",
    "and (sys_model like 'FAS8200' OR sys_model like 'AFF-A200' OR sys_model like 'AFF-A300' OR sys_model like 'FAS26%')\n",
    "and ems.event_type in('cecc_log_entry_no_syslog_1','sk_panic_1')\n",
    "and ems.tags['cecc_msg'] like 'ECC error at DIMM%'\n",
    "and ems.asup_key = mb.asup_key\n",
    "and ems.asup_key = s.asup_key '''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "que = 'drop table ems'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = '''CREATE table IF NOT EXISTS ems (asup_key string, asup_id string, tags map<string, string>, \n",
    "event_type string, dt date)'''\n",
    "conn.execute(que, async=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''insert into asuprep.ILM_XML\n",
    "select a.,\n",
    "from asuprep.ILM_XML_TEMP a where a.asup_id not in (select distinct asup_id from asuprep.ILM_XML) \n",
    "'''\n",
    "\n",
    "que = 'drop table asuprep.ILM_XML'\n",
    "#conn.execute(que, async=False)\n",
    "\n",
    "que = 'drop table asuprep.ILM_XML_TEMP'\n",
    "#conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS asuprep.ILM_XML (asup_id string, attr string,name string,value string,dt date)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS asuprep.ILM_XML_TEMP (asup_id string, attr string,name string,value string,dt date)'\n",
    "conn.execute(que, async=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
