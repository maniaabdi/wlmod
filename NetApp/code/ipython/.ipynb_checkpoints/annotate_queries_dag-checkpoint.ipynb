{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import graph_tool.all as gt\n",
    "from datetime import datetime, date, time\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "statistics_dir = '/home/maniaa/ashes/dataset/statistics/'\n",
    "stat_csv = [(statistics_dir + f) for f in listdir(statistics_dir) if (f.endswith(\".csv\") and isfile(join(statistics_dir, f)))]\n",
    "stat_csv.sort()\n",
    "stat_csv = stat_csv[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/maniaa/ashes/dataset/statistics/08-02-2018.csv\n",
      "Index(['state', 'submitTime', 'startTime', 'finishTime', 'queueTime',\n",
      "       'runTime', 'NumMaps', 'avgMapTime', 'avgReduceTime', 'avgShuffleTime',\n",
      "       'avgMergeTime', 'NumReduce', 'HDFS_INPUT_SIZE', 'HDFS_OUTPUT_SIZE',\n",
      "       'MAP_CPU_USAGE_MSEC', 'REDUCE_CPU_USAGE_MSEC', 'MAP_MEM_USAGE_B',\n",
      "       'REDUCE_MEM_USAGE_B', 'HIVE_RECORDS_IN', 'HIVE_RECORDS_OUT',\n",
      "       'HIVE_RECORDS_INTERMEDIATE', 'SLOTS_MILLIS_MAPS',\n",
      "       'SLOTS_MILLIS_REDUCES', 'TOTAL_LAUNCHED_MAPS', 'TOTAL_LAUNCHED_REDUCES',\n",
      "       'DATA_LOCAL_MAPS', 'RACK_LOCAL_MAPS', 'MILLIS_MAPS', 'MILLIS_REDUCES',\n",
      "       'VCORES_MILLIS_MAPS', 'VCORES_MILLIS_REDUCES', 'MB_MILLIS_MAPS',\n",
      "       'MB_MILLIS_REDUCES', 'PHMAP_MEM_USAGE_B', 'PHREDUCE_MEM_USAGE_B',\n",
      "       'PHPHYSICAL_MEMORY_B', 'jobid', 'job.maps', 'query', 'outputdir',\n",
      "       'scratchdir', 'sessionid', 'query.id', 'local.scratchdir', 'tmpouput',\n",
      "       'user.name', 'job', 'n_inputs', 'inputdir', 'workflow.node',\n",
      "       'workflow.id', 'workflow.dag', 'table.name', 'submit_ts'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "dt = pd.DataFrame()\n",
    "# 90th Percentile\n",
    "def build_graph(gstr):\n",
    "    wrk_edges = [e.split('>') for e in gstr.split(',')]\n",
    "    lbl_vid = {}\n",
    "    graph = {'nodes': {}, 'edges': []}\n",
    "    g = gt.Graph(directed=True)\n",
    "    v_lbl = g.new_vertex_property(\"int\")\n",
    "    for e in wrk_edges:\n",
    "        srclbl = int(e[0].split('-')[1])\n",
    "        if srclbl not in lbl_vid:\n",
    "            vsrc = g.add_vertex()\n",
    "            v_lbl[vsrc] = srclbl\n",
    "            lbl_vid[srclbl] = int(vsrc)\n",
    "            \n",
    "        if len(e) > 1:\n",
    "            trgtlbl = int(e[1].split('-')[1])\n",
    "            if trgtlbl not in lbl_vid:\n",
    "                vtgt = g.add_vertex()\n",
    "                v_lbl[vtgt] = trgtlbl\n",
    "                lbl_vid[trgtlbl] = int(vtgt)\n",
    "            g.add_edge(lbl_vid[srclbl], lbl_vid[trgtlbl])\n",
    "   \n",
    "    g.vertex_properties['label'] = v_lbl\n",
    "    return g\n",
    "    \n",
    "def num_vertices(g):\n",
    "    return g.num_vertices()\n",
    "\n",
    "def num_edges(g):\n",
    "    return g.num_edges()\n",
    " \n",
    "    \n",
    "for index, f in enumerate(stat_csv):\n",
    "    # 6Am of each day\n",
    "    print(f)\n",
    "    month, day, year = f.split('/')[-1].split('.csv')[0].split('-')\n",
    "    trace_starttime = datetime.combine(date(int(year), int(month), int(day)), time(7, 0))\n",
    "    \n",
    "    df = pd.read_csv(f)\n",
    "    df = df[df['submitTime']/1000 > datetime.timestamp(trace_starttime)]\n",
    "    \n",
    "    df = df[df['state'] == 'SUCCEEDED']\n",
    "    df['submit_ts'] = df['submitTime']//1000 - datetime.timestamp(trace_starttime);\n",
    "\n",
    "    print(df.columns)\n",
    "      \n",
    "    dt['dag'] = df.groupby('workflow.id')['workflow.dag'].agg('max').apply(build_graph)\n",
    "    dt[['query', 'submitTime']] = df.groupby('workflow.id')[['query', 'submitTime']].agg('max')\n",
    "    dt['n_v'] = dt['dag'].apply(num_vertices)\n",
    "    dt['n_e'] = dt['dag'].apply(num_edges)\n",
    "    dt.reset_index(inplace=True)\n",
    "    if index==0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.sort_values('submitTime', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = hive.connect(host='localhost').cursor()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "que = 'drop table default.system'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = '''CREATE table IF NOT EXISTS default.system \n",
    "(dt date, asup_key string, asup_subject string, sys_serial_no string, sys_is_netapp string,\n",
    "asup_id string, sys_version_fbranch string, system_id string, partner_hostname string,\n",
    "partner_system_id string,Hostname string,sys_model string,sys_version string,\n",
    "sys_operating_mode string, sys_type string, sys_domain string, additional_fields  map<string,string>)'''\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'drop table default.cluster_table'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS default.cluster_table (cluster_name string, cluster_identifier string, asup_id string, dt date, cluster_uuid string, asup_key string)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "\n",
    "que = 'drop table default.customerib'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = '''CREATE table IF NOT EXISTS default.customerib (sap_status string, not_e1000_customer_flag string, \n",
    "e1000_customer_flag string, G50_customer_flag string, A50_customer_flag string, msb_customer_flag string, \n",
    "email_address string, serial_number string, customer_id string, parent_name string, customer_name string,\n",
    "site_name string,service_geo string,site_country_name string,site_country string)'''\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'drop table default.asup'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS default.asup (asup_subject string, asup_id string, asup_type string, dt date, asup_gen_date date)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'drop table default.volume'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS default.volume (vol_name string, vol_state_asis string, asup_id string, aggr_id string, vol_compressed_kb string, dt date, vol_allocated_kb  string, additional_fields  map<string,string>, vol_is_root  string, vol_is_online  string, vol_asis_saved_kb  string, vol_asis_saved_pct  string, vol_compressed_pct string)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'drop table default.aggregate'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS default.aggregate (aggr_is_hybrid string, aggr_used_kb int, aggr_sused_kb int, dt date, asup_id string, aggr_name string)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'drop table default.cm'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS default.cm (asup_key string, object_name string, dt date, counters map<string,int>)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'drop table default.DFM_APP_INFO'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS default.DFM_APP_INFO (dt date)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'drop table default.DFM_APP_MODEL'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS default.DFM_APP_MODEL (dt date)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'drop table oci_svc_consumption_list_details'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS oci_svc_consumption_list_details (dt date, sys_serial_no string)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'drop table default.cluster_member'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS default.cluster_member (asup_id string, dt date)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'drop table system_perf_v1'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = '''CREATE table IF NOT EXISTS system_perf_v1 (ttl_perf_v1_ct int, ttl_diskkb_cp_read_ct int,\n",
    "ttl_diskkb_cp_write_ct int, ttl_diskkb_usr_ct int, asup_id string, dt date,\n",
    "ttl_disk_ops_cp_read_ct int, ttl_disk_ops_cp_write_ct int, ttl_disk_ops_usr_ct int)'''\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'drop table system_health_alert'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = '''CREATE table IF NOT EXISTS system_health_alert (asup_id string, alert_id string, alerting_resource string,\n",
    "dt date, alerting_resource_name string, asup_key string)'''\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'drop table system_fru'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = '''CREATE table IF NOT EXISTS system_fru (netapp_part_no string, serial_no string, asup_key string, fru_name string)'''\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'drop table motherboard'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = '''CREATE table IF NOT EXISTS motherboard (mb_serial_no string, mb_partno string, asup_key string)'''\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'drop table smfdbv2.cifs_share_byname'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = '''CREATE table IF NOT EXISTS smfdbv2.cifs_share_byname (vserver string, asup_id string, dt date)'''\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'drop table default.DFM_GLOBAL_STATUS'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS default.DFM_GLOBAL_STATUS (dt date)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'drop table default.DFM_GUI_CLICK'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS default.DFM_GUI_CLICK (dt date)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'drop table default.software_image'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS default.software_image (system_image_version string, asup_id string, system_image_is_current string, dt date)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'drop table default.adapter'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS default.adapter (hba_name string, asup_id string, dt date)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'drop table event_subscription'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS event_subscription (asup_id string, user_id string, dt date)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'drop table default.DFM_OBJECT'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS default.DFM_OBJECT (asup_id string, user_id string, dt date)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'drop table bart_disk_errors_f'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS bart_disk_errors_f (asup_id string,dt date)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'drop table default.DFM_PERFORMANCE_ADVISOR'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS default.DFM_PERFORMANCE_ADVISOR (asup_id string,dt date)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'drop table crm_data'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS crm_data  (system_id string,  sys_serial_no string, partners  ARRAY<string>, system_state string, customer_id string)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'drop table event_subscription'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS event_subscription  (user_id string, user_name string, crm_cust_list  ARRAY<string>,  cust_list string, event_id string, is_subscribed string)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    " \n",
    "que = 'drop table renewal_data'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS renewal_data (sys_serial_no string, system_id string, host_name string, hw_contract_id string, hw_contract_end_date date, sw_contract_id string, sw_contract_end_date date, nrd_contract_id date, nrd_contract_end_date date)'\n",
    "conn.execute(que, async=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "An exception occurred\n",
      "131 SELECT DISTINCT eu.user_id, \n",
      "                eu.user_name, \n",
      "                eu.system_id, \n",
      "                r.host_name, \n",
      "                eu.sys_serial_no, \n",
      "                r.hw_contract_id, \n",
      "                r.hw_contract_end_date, \n",
      "                r.sw_contract_id, \n",
      "                r.sw_contract_end_date, \n",
      "                r.nrd_contract_id, \n",
      "                r.nrd_contract_end_date \n",
      "FROM            ( \n",
      "                                SELECT DISTINCT e.system_id, \n",
      "                                                e.sys_serial_no, \n",
      "                                                u.user_id, \n",
      "                                                u.user_name \n",
      "                                FROM            ( \n",
      "                                                       SELECT system_id, \n",
      "                                                              sys_serial_no, \n",
      "                                                              partner, \n",
      "                                                              system_state,\n",
      "    \t\t\t\t\t\t\t\t\t\t\t\t\t\t  customer_id\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  \n",
      "                                                       FROM   crm_data LATERAL view explode(partners) p as partner) e,\n",
      "                                                ( \n",
      "                                                       SELECT user_id, \n",
      "                                                              user_name, \n",
      "                                                              crmcustomerid,\n",
      "                                                              cust_list\n",
      "                                                       FROM   event_subscription LATERAL view explode(crm_cust_list) p as crmcustomerid\n",
      "                                                       WHERE  event_id = '3' \n",
      "                                                       AND    is_subscribed = 'true') u \n",
      "                                WHERE           e.partner = u.crmcustomerid and array_contains(u.cust_list,e.customer_id)\n",
      "                                AND             e.system_state = 'ACTIVE' and u.user_id='lislegaard') eu, \n",
      "                renewal_data r \n",
      "WHERE           eu.sys_serial_no = r.sys_serial_no \n",
      "AND             eu.system_id = r.system_id\n",
      "AND ( (r.hw_contract_end_date BETWEEN '2018-08-01 05:50:31.642' AND '2019-02-01 05:50:31.642') OR (r.sw_contract_end_date BETWEEN '2018-08-01 05:50:31.642' AND '2019-02-01 05:50:31.642') OR (r.nrd_contract_end_date BETWEEN '2018-08-01 05:50:31.642' AND '2019-02-01 05:50:31.642')) \n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "Operational Error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-245c78bb99fd>\u001b[0m in \u001b[0;36moperator_footprint\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"explain FORMATTED \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mexec_plan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetchall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyhive/hive.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, operation, parameters, **kwargs)\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExecuteStatement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m         \u001b[0m_check_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_operationHandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moperationHandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyhive/hive.py\u001b[0m in \u001b[0;36m_check_status\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatusCode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mttypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTStatusCode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSUCCESS_STATUS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mOperationalError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=['*org.apache.hive.service.cli.HiveSQLException:Error while compiling statement: FAILED: UDFArgumentException explode() takes an array or a map as a parameter:28:27', 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:316', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:112', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:181', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:257', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:388', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatement:HiveSessionImpl.java:369', 'sun.reflect.GeneratedMethodAccessor52:invoke::-1', 'sun.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:498', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:-2', 'javax.security.auth.Subject:doAs:Subject.java:422', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1844', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'com.sun.proxy.$Proxy20:executeStatement::-1', 'org.apache.hive.service.cli.CLIService:executeStatement:CLIService.java:262', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:490', 'org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1313', 'org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1298', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:39', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:39', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:285', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1149', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:624', 'java.lang.Thread:run:Thread.java:748', '*org.apache.hadoop.hive.ql.exec.UDFArgumentException:explode() takes an array or a map as a parameter:50:23', 'org.apache.hadoop.hive.ql.udf.generic.GenericUDTFExplode:initialize:GenericUDTFExplode.java:73', 'org.apache.hadoop.hive.ql.udf.generic.GenericUDTF:initialize:GenericUDTF.java:56', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:genUDTFPlan:SemanticAnalyzer.java:6947', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:genSelectPlan:SemanticAnalyzer.java:3887', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:genLateralViewPlan:SemanticAnalyzer.java:9912', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:genLateralViewPlans:SemanticAnalyzer.java:9856', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:genPlan:SemanticAnalyzer.java:9711', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:genPlan:SemanticAnalyzer.java:9636', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:genPlan:SemanticAnalyzer.java:9663', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:genPlan:SemanticAnalyzer.java:9636', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:genPlan:SemanticAnalyzer.java:9663', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:genPlan:SemanticAnalyzer.java:9649', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:genOPTree:SemanticAnalyzer.java:10122', 'org.apache.hadoop.hive.ql.parse.CalcitePlanner:genOPTree:CalcitePlanner.java:326', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:analyzeInternal:SemanticAnalyzer.java:10133', 'org.apache.hadoop.hive.ql.parse.CalcitePlanner:analyzeInternal:CalcitePlanner.java:210', 'org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer:analyze:BaseSemanticAnalyzer.java:227', 'org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer:analyzeInternal:ExplainSemanticAnalyzer.java:74', 'org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer:analyze:BaseSemanticAnalyzer.java:227', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:425', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:309', 'org.apache.hadoop.hive.ql.Driver:compileInternal:Driver.java:1145', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1139', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:110'], sqlState='42000', errorCode=40000, errorMessage='Error while compiling statement: FAILED: UDFArgumentException explode() takes an array or a map as a parameter'), operationHandle=None)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-245c78bb99fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m \u001b[0mdt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'operators'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'query'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperator_footprint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   3846\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3847\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3848\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3850\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-91-245c78bb99fd>\u001b[0m in \u001b[0;36moperator_footprint\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"An exception occurred\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNameError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Operational Error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'create table AFF_VOLUME as'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: Operational Error"
     ]
    }
   ],
   "source": [
    "from pyhive import hive\n",
    "import ast\n",
    "\n",
    "def process_tree(tree):\n",
    "    if not tree: return ''\n",
    "    for op in tree:\n",
    "        if 'children' not in tree[op]: \n",
    "            return op.replace(' Operator', '')\n",
    "        return op.replace(' Operator', '') + ' | ' + process_tree(tree[op]['children'])\n",
    "        \n",
    "\n",
    "def process_execution_plan(exp):\n",
    "    operations = {}\n",
    "    for stage_type in exp:\n",
    "        if stage_type == 'Map Reduce':\n",
    "            map_trees = exp[stage_type]['Map Operator Tree:']\n",
    "            reduce_tree = exp[stage_type]['Reduce Operator Tree:']\n",
    "            \n",
    "            for tree in map_trees:\n",
    "                operations['map'] = process_tree(tree)\n",
    "            operations['reduce'] = process_tree(reduce_tree)\n",
    "    return operations\n",
    "\n",
    "def operator_footprint(query):\n",
    "    global i    \n",
    "    i = i + 1\n",
    "    \n",
    "    if 'dt = 2018' in query:\n",
    "        return;\n",
    "    \n",
    "    try:\n",
    "        print(i)\n",
    "        conn.execute(\"explain FORMATTED \" + query, async=False)\n",
    "        exec_plan = conn.fetchall()\n",
    "    except:\n",
    "        print(\"An exception occurred\")\n",
    "        print(i, query, '\\n')\n",
    "        raise NameError(\"Operational Error\")\n",
    "    \n",
    "    if 'create table AFF_VOLUME as' in query:\n",
    "        que = 'drop table AFF_VOLUME'\n",
    "        conn.execute(que, async=False)\n",
    "        \n",
    "        que = 'CREATE table IF NOT EXISTS AFF_VOLUME ((asup_id string,sys_serial_no string,sys_version string,aggr_id string,vol_name string, vol_state_asis string, vol_compressed_kb int, guarantee  string, vol_is_vsroot  string, vol_is_root string,vol_is_online string,vol_asis_saved_kb int, vol_asis_saved_pct int, vol_compressed_pct int, Aggregate_Used_KB int, Vol_size int)'\n",
    "        conn.execute(que, async=False)\n",
    "    \n",
    "    \n",
    "    if \"create table OntapAdoptionDevice as\" in query:\n",
    "        que = 'drop table OntapAdoptionDevice'\n",
    "        conn.execute(que, async=False)\n",
    "        \n",
    "        que = 'CREATE table IF NOT EXISTS OntapAdoptionDevice (asup_key string, asup_id string, dt date, dvc_disk_type string, dvc_serial_no string, dvc_phy_size_mb string, dvc_type string, dvc_label string)'\n",
    "        conn.execute(que, async=False)\n",
    "\n",
    "    #print(exec_plan)\n",
    "    #stages = {}\n",
    "    #for stgid in exec_plan['STAGE PLANS']:\n",
    "    #    operations = process_execution_plan(exec_plan['STAGE PLANS'][stgid])\n",
    "    #    if operations:\n",
    "    #        stages[stgid] = operations\n",
    "    \n",
    "    #print(json.dumps(stages, indent=4))\n",
    "    #return stages\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "i=0;\n",
    "dt['operators'] = dt['query'].apply(operator_footprint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "create table test_nb_db.ib_master_s as\n",
    "select a.SYS_SERIAL_NO,a.IOBJECTID_SAP,a.SYSTEMID_SAP,a.HOSTNAME_SAP,a.SYS_PRODID_SAP,a.SYS_VERSION,\n",
    "cast(To_date(from_unixtime(unix_timestamp(a.SYS_SHIPDATE_SAP, 'yyyy-MM-dd'))) as date) SYS_SHIPDATE_SAP,\n",
    "cast(To_date(from_unixtime(unix_timestamp(a.FIRST_ASUP_DAY, 'yyyy-MM-dd'))) as date) FIRST_ASUP_DAY,\n",
    "cast(To_date(from_unixtime(unix_timestamp(a.LAST_ASUP_DAY, 'yyyy-MM-dd'))) as date) LAST_ASUP_DAY,\n",
    "a.CUSTOMERID_SAP,a.SITEID_SAP,a.CUST_ACCTYPE_SAP,a.CUST_NAME_SAP,a.CUST_NAGP,\n",
    "a.CUST_NAME,a.CUST_SITENAME_SAP,a.CUST_DESCR_SAP,a.SYS_STATUS_SAP,a.SYS_ASUPSTATUS_SAP,a.MKT_CLASSIF_SAP,\n",
    "a.MKT_SEGMENT_SAP,a.NA_CMAT_ID_SAP,a.PARENTID_SAP,a.AUTHORIZATIONGRP_SAP,\n",
    "cast(To_date(from_unixtime(unix_timestamp(a.ETL_DATE, 'yyyy-MM-dd'))) as date) ETL_DATE,\n",
    "a.SYS_MODEL,a.SYSTEM_NAME,a.SYS_MODE,a.SALES_ORDER_NUMBER,a.SALES_CHANNEL_TYPE,\n",
    "cast(To_date(from_unixtime(unix_timestamp(a.SHIPPED_DATE, 'yyyy-MM-dd'))) as date) SHIPPED_DATE,\n",
    "cast(To_date(from_unixtime(unix_timestamp(a.INSTALLED_DATE, 'yyyy-MM-dd'))) as date) INSTALLED_DATE,a.MANUAL_INSTALLED_DATE_FLAG,\n",
    "cast(To_date(from_unixtime(unix_timestamp(a.LAST_ASUP_DATE, 'yyyy-MM-dd'))) as date) LAST_ASUP_DATE,\n",
    "a.ASUP_STATUS,a.CUST_E1000,a.CUST_SITE,a.CUST_CITY,a.CUST_STATE,a.CUST_COUNTRY,\n",
    "a.CUST_ACC_CLASSIFICATION,a.INSTALLED_BASE_GROUP,a.INSTALLED_PROD_STATUS,a.SERIAL_NO_OWNER_NAGP,a.SERIAL_NO_OWNER_NAME,\n",
    "a.TGA_FLAG,a.HA_PAIR_FLAG,a.CUST_RESELLER_PARTY_NAME,a.CUST_ASP_NAME,a.CUST_SP_NAME,a.CUST_TPM_NAME,a.CUST_SALES_GEO,\n",
    "a.CUST_SERVICE_GEO,a.CUST_SERVICE_AREA,a.SYS_NETAPP,a.SYS_NAME,a.ID1,a.SERVICE_LEVEL,\n",
    "cast(To_date(from_unixtime(unix_timestamp(a.SYS_START_DATE, 'yyyy-MM-dd'))) as date) SYS_START_DATE,\n",
    "cast(To_date(from_unixtime(unix_timestamp(a.SYS_END_DATE, 'yyyy-MM-dd'))) as date) SYS_END_DATE,\n",
    "a.CUST_SEA,a.CUST_SEA_GEO,a.CUST_SAM,a.TOP_CUST_NAME,a.CUST_G50,a.MISSING_FLAG,a.CUST_G100,a.SYS_ASP,a.SALES_DISTRICT,\n",
    "a.CUST_NAGP_OLD,a.CUST_NAME_OLD,a.SYS_FAMILY,a.SYS_TOPFAMILY,a.SYS_FAMILYNAME,a.SYS_PLATFORM_RANGE,a.SYS_TYPE,\n",
    "a.CUST_G100_OLD,  case when (a.cust_name is null) then 0 else 1 end as cust_name_flg \n",
    "from test_nb_db.IB_MASTER_RAW a \n",
    "'''\n",
    "\n",
    "que = 'CREATE database test_nb_db'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS test_nb_db.IB_MASTER_RAW  (SYS_SERIAL_NO  int, IOBJECTID_SAP int, SYSTEMID_SAP int, HOSTNAME_SAP string, SYS_PRODID_SAP string, SYS_VERSION string, SYS_SHIPDATE_SAP DATE, FIRST_ASUP_DAY date, LAST_ASUP_DAY date, CUSTOMERID_SAP string, SITEID_SAP string, CUST_ACCTYPE_SAP string, CUST_NAME_SAP string, CUST_NAGP string, CUST_NAME string, CUST_SITENAME_SAP string, CUST_DESCR_SAP string, SYS_STATUS_SAP string, SYS_ASUPSTATUS_SAP string, MKT_CLASSIF_SAP string, MKT_SEGMENT_SAP string, NA_CMAT_ID_SAP string, PARENTID_SAP string, AUTHORIZATIONGRP_SAP string, ETL_DATE date, SYS_MODEL string, SYSTEM_NAME string, SYS_MODE string, SALES_ORDER_NUMBER string, SALES_CHANNEL_TYPE string, SHIPPED_DATE date, INSTALLED_DATE date, MANUAL_INSTALLED_DATE_FLAG date, LAST_ASUP_DATE date, ASUP_STATUS date, CUST_E1000 int, CUST_SITE string, CUST_CITY string, CUST_STATE string, CUST_COUNTRY string, CUST_ACC_CLASSIFICATION string, INSTALLED_BASE_GROUP string, INSTALLED_PROD_STATUS string, SERIAL_NO_OWNER_NAGP string, SERIAL_NO_OWNER_NAME string, TGA_FLAG string, HA_PAIR_FLAG string, CUST_RESELLER_PARTY_NAME string, CUST_ASP_NAME string, CUST_SP_NAME string, CUST_TPM_NAME string, CUST_SALES_GEO string, CUST_SERVICE_GEO string, CUST_SERVICE_AREA string, SYS_NETAPP string, SYS_NAME string, ID1 string, SERVICE_LEVEL string,  SYS_START_DATE string, SYS_END_DATE string, CUST_SEA string, CUST_SEA_GEO string, CUST_SAM string, TOP_CUST_NAME string, CUST_G50 string, MISSING_FLAG string, CUST_G100 string, SYS_ASP string, SALES_DISTRICT string, CUST_NAGP_OLD string, CUST_NAME_OLD string, SYS_FAMILY string, SYS_TOPFAMILY string, SYS_FAMILYNAME string, SYS_PLATFORM_RANGE string, SYS_TYPE string, CUST_G100_OLD  string)'\n",
    "conn.execute(que, async=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''insert into UBS_SHELF_ASUPS_SD \n",
    "SELECT hc.* FROM default.hdfs_latest_calmonth hc,UBS_SHELF_ASUPS_1 b\n",
    "where hc.dt = '2156-10-17'\n",
    "and get_json_object(value, \"$.fields.asupId\")=b.asup_id''' \n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS UBS_SHELF_ASUPS_SD (dt date, asup_id string)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS default.hdfs_latest_calmonth (dt date, asup_id string)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS UBS_SHELF_ASUPS_1 (dt date, asup_id string)'\n",
    "conn.execute(que, async=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "insert into asuprep.summary_attributes\n",
    "select a.asup_id,\n",
    "oid,\n",
    "path,\n",
    "attr,\n",
    "name,\n",
    "value,\n",
    "index,\n",
    "tableid,\n",
    "tablename,\n",
    "dt from asuprep.summary_attributes_temp a\n",
    "where a.asup_id not in (select distinct asup_id from asuprep.summary_attributes)\n",
    "'''\n",
    "\n",
    "\n",
    "que = 'CREATE database asuprep'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS asuprep.summary_attributes (asup_id string, oid string, path string, attr string, name string, value string, index string, tableid string, tablename string, dt date)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS asuprep.summary_attributes_temp (asup_id string, oid string, path string, attr string, name string, value string, index string, tableid string, tablename string, dt date)'\n",
    "conn.execute(que, async=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "create table OntapAdoptionDevice as\n",
    "select a.* from default.device a,(\n",
    "select split(asup_key,'\\\\|')[3] serial,max(asup_id) asup_id \n",
    "from default.device where dt>='20180601' and dt<='20180802'\n",
    "group by split(asup_key,'\\\\|')[3]) b\n",
    "where a.asup_id=b.asup_id \n",
    "'''\n",
    "\n",
    "'''\n",
    "create table OntapAdoptionDevice_final as\n",
    "select split(asup_key,'\\\\|')[3] serial,dvc_disk_type,count(distinct dvc_serial_no),SUM(dvc_phy_size_mb)\n",
    "from OntapAdoptionDevice where dvc_type IN ('DISK','LUN') and dvc_label<>'PARTNER'\n",
    "group by split(asup_key,'\\\\|')[3] ,dvc_disk_type \n",
    "'''\n",
    "\n",
    "que = 'drop table default.device'\n",
    "#conn.execute(que, async=False)\n",
    "\n",
    "que = 'drop table OntapAdoptionDevice'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS default.device (asup_key string, asup_id string, dt date, dvc_disk_type string, dvc_serial_no string, dvc_phy_size_mb string, dvc_type string, dvc_label string)'\n",
    "#conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS OntapAdoptionDevice (asup_key string, asup_id string, dt date, dvc_disk_type string, dvc_serial_no string, dvc_phy_size_mb string, dvc_type string, dvc_label string)'\n",
    "#conn.execute(que, async=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Insert into application_record_ocum \n",
    "select a.asup_id, \n",
    "  get_json_object(value,'$.id') as id, \n",
    "  get_json_object(value,'$.url') as url, \n",
    "  get_json_object(value,'$.version') as version, \n",
    "  get_json_object(value,'$.date_added') as date_added, \n",
    "  get_json_object(value,'$.platform') as platform, \n",
    "  get_json_object(value,'$.hostFQDN') as hostFQDN, \n",
    "  get_json_object(value,'$.lastCheckin') as lastCheckin,\n",
    "  get_json_object(value,'$.modified_from') as modified_from\n",
    "from smfdbv2.application_record a join (select max(asup_id) asup_id from application_record_ocum) b\n",
    "where UPPER(name) like '%UNIFIED%'\n",
    "and a.asup_id > b.asup_id \n",
    "'''\n",
    "\n",
    "que = 'CREATE database smfdbv2'\n",
    "#conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS smfdbv2.application_record (value string, asup_id string, dt date, name string)'\n",
    "#conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS application_record_ocum (id string, url string, version string, date_added date, platform string, hostFQDN string, lastCheckin date, modified_from string, asup_id string)'\n",
    "#conn.execute(que, async=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "create table ocum_ontap as\n",
    "\tselect arc.asup_id as ontap_asup_id,\n",
    "\t\t   a.asup_gen_date,\n",
    "\t\t   arc.id as ocum_system_uuid, \n",
    "\t\t   arc.url as ocum_url, \n",
    "\t\t   arc.version as ocum_version, \n",
    "\t\t   arc.date_added as ocum_date_added, \n",
    "\t\t   arc.platform as ocum_platform, \n",
    "\t\t   arc.hostFQDN as ocum_hostFQDN, \n",
    "\t\t   arc.lastCheckin as ocum_lastCheckin, \n",
    "\t\t   arc.modified_from as modified_from,\n",
    "\t\t   ct.cluster_name,\n",
    "\t\t   ct.cluster_identifier as cluster_uuid,\n",
    "\t\t   sc.serial_number as node_serial_number,\n",
    "\t\t   sc.system_id as ontap_system_id,\n",
    "\t\t   sc.partner_system_id as ontap_partner_system_id,\n",
    "\t\t   sc.Hostname as ontap_hostname,\n",
    "\t\t   sc.sys_model,\n",
    "\t\t   sc.sys_version,\n",
    "\t\t   sc.sys_operating_mode,\n",
    "\t\t   sc.customer_id,\n",
    "\t\t   sc.customer_name,\n",
    "\t\t   sc.site_name,\n",
    "\t\t   sc.service_geo,\n",
    "\t\t   sc.site_country_name as site_city,\n",
    "\t\t   sc.site_country,\n",
    "\t\t   sc.internal_netapp_flag,\n",
    "\t\t   sc.ha_configured,\n",
    "\t\t   sc.is_aff_ct,\n",
    "\t\t   sc.is_mcc_ct\n",
    "\tfrom asuprep.application_record_ocum arc\n",
    "\tjoin (select distinct * from default.cluster_table) ct on arc.asup_id = ct.asup_id\n",
    "\tjoin (select distinct * from default.asup) a on arc.asup_id = a.asup_id\n",
    "\tjoin\n",
    "\t(\n",
    "\t  select sy.asup_id,\n",
    "\t\t\t sy.system_id,\n",
    "\t\t\t sy.partner_system_id,\n",
    "\t\t\t sy.Hostname,\n",
    "\t\t\t sy.sys_model,\n",
    "\t\t\t sy.sys_version,\n",
    "\t\t\t sy.sys_operating_mode,\n",
    "\t\t\t cib.customer_id,\n",
    "\t\t\t cib.customer_name,\n",
    "\t\t\t cib.site_name,\n",
    "\t\t\t cib.serial_number,\n",
    "\t\t\t cib.service_geo,\n",
    "\t\t\t cib.site_country_name,\n",
    "\t\t\t cib.site_country,\n",
    "\t\t\t case when ((cib.Customer_Name not like '%NETAPP%' and cib.Customer_Name not like '%NTAP%' and cib.Customer_Name not like '%Network Appliance%' \n",
    "\t\t\t\t\t\t and cib.Customer_Name not like '%NetworkAppliance%' AND cib.Customer_Name not like '%NetApp%' and cib.Customer_Name not like '%Ntap%' \n",
    "\t\t\t\t\t\t and cib.Customer_Name not like '%Netapp%') and sy.Sys_Model <> ' SIMBOX' ) \n",
    "\t\t\t\t  then 'N'\n",
    "\t\t\t\t  else 'Y' \n",
    "\t\t\t end as internal_netapp_flag,\n",
    "\t\t\t case when ( sy.additional_fields['is_mcc'] is null) \n",
    "\t\t\t\t  then 'False'\n",
    "\t\t\t\t  else sy.additional_fields['is_mcc']\n",
    "\t\t\t end is_mcc_ct,\n",
    "\t\t\t case when ( sy.additional_fields['is_all_flash_optimized'] is null) \n",
    "\t\t\t\t  then 'False'\n",
    "\t\t\t\t  else sy.additional_fields['is_all_flash_optimized']\n",
    "\t\t\t end is_aff_ct,\n",
    "\t\t\t case when ( sy.additional_fields['ha_mode'] is null) \n",
    "\t\t\t\t  then 'False'\n",
    "\t\t\t\t  else sy.additional_fields['ha_mode']\n",
    "\t\t\t end ha_configured\n",
    "\t  from (select distinct * from default.system) as sy \n",
    "\t  join default.customerib as cib on  sy.sys_serial_no=cib.serial_number \n",
    "\t) sc on arc.asup_id = sc.asup_id \n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS asuprep.application_record_ocum (id string, url string, version string, date_added date, platform string, hostFQDN string, lastCheckin date, modified_from string, asup_id string)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS default.system (asup_subject string, sys_serial_no string, asup_id string, system_id string, partner_system_id string,Hostname string,sys_model string,sys_version string,sys_operating_mode string, additional_fields  map<string,string>)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'INSERT INTO TABLE default.cluster_table VALUES (\"1\", \"1111\", \"1111\")'\n",
    "conn.execute(que, async=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "create table ontapadoptionsystem as\n",
    "select a.asup_id asup_id,s.additional_fields[\"is_all_flash_optimized\"] Aff \n",
    "from default.asup a, default.system s where a.asup_id=s.asup_id and a.asup_type='DOT-REGULAR' \n",
    "and a.dt>='20180601' and a.dt<='20180802' and\n",
    "s.dt>='20180601' and s.dt<='20180802'\n",
    "'''\n",
    "\n",
    "\n",
    "que = 'drop table default.asup'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS default.asup (asup_id string, asup_gen_date date, dt date, additional_fields  map<string,string>)'\n",
    "conn.execute(que, async=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''create table ontapadoptionprotocol as\n",
    "select split(asup_key,'\\\\|')[3] serial,max(tags[\"cifsOps\"]) CIFS_OPS,\n",
    "max(tags[\"httpOps\"]) HTTP_OPS,max(tags[\"nfsOps\"]) NFS_OPS,\n",
    "max(tags[\"fcpOps\"]) FCP_OPS,\n",
    "max(tags[\"iscsiOps\"]) ISCSI_OPS\n",
    "from default.ems a,(\n",
    "select split(asup_key,'\\\\|')[3] serial,max(tags[\"asup_id\"]) asup_id\n",
    "from default.ems where dt>='2018-06-01' and dt<='2018-08-02' \n",
    "and event_type='kern_uptime_filer_1' and (tags[\"cifsOps\"] <>0 or tags[\"cifsOps\"] IS NOT NULL or tags[\"fcpOps\"] <>0 or tags[\"fcpOps\"] IS NOT NULL or tags[\"nfsOps\"] <>0 or tags[\"nfsOps\"] IS NOT NULL \n",
    "    or tags[\"iscsiOps\"] <>0 or tags[\"iscsiOps\"] IS NOT NULL or tags[\"httpOps\"] <>0 or tags[\"httpOps\"] IS NOT NULL)\n",
    "group by split(asup_key,'\\\\|')[3]) b\n",
    "where a.tags[\"asup_id\"]=b.asup_id\n",
    "and a.dt>='2018-06-01' and a.dt<='2018-08-02'\n",
    "group by split(asup_key,'\\\\|')[3] '''\n",
    "\n",
    "que = 'drop table ems'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS ems (asup_key string, asup_id string, tags map<string, string>, event_type string, dt date)'\n",
    "conn.execute(que, async=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''insert into asuprep.ILM_XML\n",
    "select a.,\n",
    "from asuprep.ILM_XML_TEMP a where a.asup_id not in (select distinct asup_id from asuprep.ILM_XML) \n",
    "'''\n",
    "\n",
    "que = 'drop table asuprep.ILM_XML'\n",
    "#conn.execute(que, async=False)\n",
    "\n",
    "que = 'drop table asuprep.ILM_XML_TEMP'\n",
    "#conn.execute(que, async=False)\n",
    "\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS asuprep.ILM_XML (asup_id string, attr string,name string,value string,dt date)'\n",
    "conn.execute(que, async=False)\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS asuprep.ILM_XML_TEMP (asup_id string, attr string,name string,value string,dt date)'\n",
    "conn.execute(que, async=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''create table AFF_VOLUME as\n",
    "select a.asup_id,sys_serial_no,sys_version,aggr_id,vol_name,vol_state_asis,vol_compressed_kb,\n",
    "a.additional_fields[\"vol_space_guarantee\"] guarantee,\n",
    "a.additional_fields[\"vol_is_vsroot\"] vol_is_vsroot,\n",
    "vol_is_root,vol_is_online,a.vol_asis_saved_kb,a.vol_asis_saved_pct,a.vol_compressed_pct,\n",
    "c.aggr_used_kb+c.aggr_sused_kb Aggregate_Used_KB, vol_allocated_kb Vol_size\n",
    "from default.volume a,default.system b ,default.aggregate c\n",
    "where a.asup_id=b.asup_id\n",
    "and b.asup_id=c.asup_id\n",
    "and a.aggr_id=c.aggr_name\n",
    "and a.dt>=regexp_replace(date_add(from_unixtime(unix_timestamp()),-29),'-','')\n",
    "and b.dt>=regexp_replace(date_add(from_unixtime(unix_timestamp()),-29),'-','')\n",
    "and c.dt>=regexp_replace(date_add(from_unixtime(unix_timestamp()),-29),'-','')\n",
    "and b.additional_fields[\"is_all_flash_optimized\"]='true' '''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''create table AFF_VOLUME as\n",
    "select (asup_id string,sys_serial_no string,sys_version string,aggr_id string,vol_name string, vol_state_asis string, vol_compressed_kb int, guarantee  string, vol_is_vsroot  string, vol_is_root string,vol_is_online string,vol_asis_saved_kb int, vol_asis_saved_pct int, vol_compressed_pct int, Aggregate_Used_KB int, Vol_size int) '''\n",
    "\n",
    "'''create table AFF_VOLUME_1 as\n",
    "select a.*\n",
    "from AFF_VOLUME a,(select sys_serial_no,max(asup_id) asup_id from AFF_VOLUME group by sys_serial_no) b \n",
    "where a.asup_id=b.asup_id '''\n",
    "\n",
    "que = 'CREATE table IF NOT EXISTS AFF_VOLUME (sys_serial_no string, asup_id string)'\n",
    "conn.execute(que, async=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
